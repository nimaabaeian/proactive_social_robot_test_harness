# Embodied Behaviour & Learning Modules Documentation

## Table of Contents
1. [System Overview](#system-overview)
2. [Console Dashboard](#console-dashboard)
3. [Architecture](#architecture)
4. [Embodied Behaviour Module](#embodied-behaviour-module)
5. [Learning Module](#learning-module)
6. [Communication Protocol](#communication-protocol)
7. [Configuration & Parameters](#configuration--parameters)
8. [Data Logging](#data-logging)
9. [Reward Function Design](#reward-function-design)
10. [Learning Progress & Convergence](#learning-progress--convergence)

---

## System Overview

### Purpose
This system implements a **developmental reinforcement learning architecture** for social robot interaction. The robot learns which social behaviors (greetings, gestures, etc.) are most effective in different contexts through experience.

### Core Concept
- **Actor-Critic Architecture**: Two independent modules work together
  - **Embodied Behaviour** (Actor): Perceives, decides, and acts
  - **Learning** (Critic): Evaluates actions and improves decision-making

### Module Summary

#### Embodied Behaviour (Actor)
**Purpose**: Perceive environment, make decisions, execute actions

**6 Threads**:
1. **IIE Monitor**: Tracks interaction intention (mean, variance) from face analysis
2. **Context Monitor**: Tracks environment classification (calm=0, lively=1)
3. **Info Monitor**: Tracks face count and mutual gaze
4. **Always-On Monitor**: Auto-stops system after 120s with no faces, restarts when faces return
5. **Proactive Thread**: Learning-driven actions with threshold checks (IIE‚â•0.5, var<0.1, gaze>0)
   - Uses epsilon-greedy selection from Q-table
   - Collects pre/post state with 3s blocking snapshots
   - Sends 13-field experience to Learning module
6. **Self-Adaptor Thread**: Random background behaviors (yawn, look around, cough)
   - Random 60-120s periods
   - Yields to proactive actions (priority system)
   - Only executes when always-on is active

**Key Mechanism**: Blocking windowed snapshots collect 30 samples over 3s at 0.1s intervals for noise-robust state measurement

#### Learning (Critic)
**Purpose**: Evaluate actions and improve policy

**2 Learning Systems**:
1. **Q-Learning**: TD(0) updates with multi-component reward
   - Structure: Q[context][action] ‚Üí value
   - Reward = W_delta√ó(IIE_change) + W_var√ó(var_reduction) + W_level√ó(threshold_margin) - action_cost
   - Updates: Œ±=0.3, Œ≥=0.92, clipped to [-1,1]
   - Saves JSON Q-table after every experience

2. **Gatekeeper (Scene Discriminator)**: GradientBoostingClassifier
   - Learns from raw outcomes: label = 1 if (post_IIE - pre_IIE) > 0.02 else 0
   - Input: 6D pre-state features [pre_IIE_mean, pre_IIE_var, pre_ctx, pre_faces, pre_gaze, time_delta]
   - Output: Binary (YES/NO) - should act in this scene?
   - Buffer size: 4 samples, grows from 100‚Üí200 trees
   - Currently disabled in embodiedBehaviour.py (Phase 3 feature)

**Key Distinction**: Q-learning uses weighted reward function; gatekeeper learns from physical reality (simple post-pre comparison)

### Key Features
- **Real-time learning**: Updates after every interaction
- **Context-aware**: Adapts behavior to calm vs. lively environments
- **Noise-robust**: Filters out measurement jitter with dead zones and blocking windowed snapshots (3s averaging at 0.1s steps)
- **Exploration-exploitation**: Balances trying new things vs. using known strategies
- **Always-on autonomy**: Auto-stops when no one is present, restarts when people return
- **Self-adaptor behaviors**: Natural background actions (yawn, look around, cough) with proactive priority
- **Subprocess RPC**: Fire-and-forget action execution via shell commands

### Architecture Design

**1. Blocking Windowed Snapshots**
- State captured on-demand via `_windowed_snapshot(duration=3.0, step=0.1)`
- Collects ~30 samples over 3 seconds for noise reduction
- Pre-state: 3s blocking collection
- Post-state: 3s blocking collection
- Total measurement overhead: 6s per action (necessary for stable readings)

**2. Subprocess-Based RPC**
- Uses `subprocess.run('echo "exe {cmd}" | yarp rpc /interactionInterface')`
- Fire-and-forget pattern: no reply checking needed
- 5-second timeout for command execution
- 
**3. Self-Adaptor Priority System**
- Self-adaptors execute only when always-on is active
- Random period between 60-120 seconds per cycle (1-2 minutes)
- Proactive actions have priority: self-adaptor aborts cycle if proactive starts
- Aborts only after proactive passes threshold checks and selects action
- Restarts with new random period after proactive completes

**4. Optimized Timing**
- `WAIT_AFTER_ACTION`: **3.0s** (action execution + human reaction time)
- `COOLDOWN`: **5.0s** (minimum time between proactive actions)
- Pre-state collection: **3.0s** (blocking windowed snapshot)
- Post-state collection: **3.0s** (blocking windowed snapshot)
- Total cycle time: **~14s per action** (3s pre + 3s wait + 3s post + 5s cooldown)

**Why Blocking Snapshots?**
- Action execution time: ~0.5s
- Human reaction time: ~1.0-2.0s
- Data collection: 3.0s (30 samples at 0.1s intervals)
- The blocking approach ensures synchronized measurements
- Simpler implementation without rolling buffer complexity

---

## Console Dashboard

### Purpose
The system provides **real-time visual feedback** through a clean, intuitive console interface. All messages use consistent prefixes and emojis for instant status recognition.

### Logging Convention

**Module Prefixes**:
- `[Actor]` - Embodied Behaviour module (perception, decision-making, action execution)
- `[Actor/IIE]` - IIE Monitor thread
- `[Actor/CTX]` - Context Monitor thread
- `[Actor/INFO]` - Info Monitor thread (faces/gaze)
- `[Actor/AO]` - Always-On Monitor thread
- `[Actor/PRO]` - Proactive thread (learning actions)
- `[Actor/SA]` - Self-Adaptor thread (background behaviors)
- `[Learner]` - Learning module (critic)
- `[Learner/Q]` - Q-Learning updates
- `[Learner/Gate]` - Gatekeeper training

**Status Emojis**:
- ‚úÖ **Success** / Threshold Met / Action Accepted
- ‚è∏Ô∏è **Waiting** / Threshold Not Met / Paused
- üö´ **Gatekeeper Rejection** / Scene Not Opportune
- ‚ö° **Action Execution** / Command Sent
- üì• **Receiving Data** / Experience Received
- üì§ **Sending Data** / Experience Sent
- üíæ **Saving Files** / Model Persistence
- üìä **Measurements** / State Updates
- üé≤ **Exploration** / Epsilon Decay
- üîÑ **Background Action** / Self-Adaptor
- ‚ùå **Error** / Failed Operation
- üõë **Shutdown** / Module Closing
- ‚ñ∂Ô∏è **Thread Started**
- ‚èπÔ∏è **Thread Stopped**

### Example Output

**Startup**:
```
[Actor] ü§ñ EMBODIED BEHAVIOUR MODULE
[Actor] ‚úÖ Ports ready
[Actor] ‚úÖ Always-On active
[Actor] ‚úÖ 6 threads started
[Actor] üìä Thresholds: Œº‚â•0.5, œÉ¬≤<0.1, Œµ=0.80

[Learner] üß† LEARNING MODULE
[Learner] üìÅ Paths: Q=learning_qtable.json, Gate=gate_classifier.pkl
[Learner] ‚úÖ Port ready
[Learner] üíæ Q-table: 2 states
[Learner] üíæ Gatekeeper: 150 trees
[Learner] üìä Q-Learning: Œ±=0.30, Œ≥=0.92
[Learner] üéØ Gatekeeper: Trained (100 trees)
```

**Monitoring Activity**:
```
[Actor/IIE] üìä Œº=0.62, œÉ¬≤=0.08
[Actor/CTX] üî¥Lively
[Actor/INFO] üë§2 üëÅÔ∏è1
```

**Action Decision Flow**:
```
[Actor/PRO] ‚úÖ Thresholds: Œº=0.62, œÉ¬≤=0.08
[Actor/PRO] ‚ö° ao_greet | CTX1 Œº=0.62 Q=0.45 Œµ=0.72
[Actor/PRO] üìä Result: Œº 0.62‚Üí0.68 (+0.06)
[Actor/PRO] üì§ Sent to Learner
```

**Learning Updates**:
```
[Learner] üì• ao_greet
[Learner] Pre‚ÜíPost: Œº 0.62‚Üí0.68, CTX1‚Üí1
[Learner/Q] ‚úÖ R=+0.18, Q: 0.45‚Üí0.51, TD=+0.06
[Learner/Gate] ‚úÖYES Œî=+0.06
[Learner/Gate] üéâ Initialized (100 trees)
[Learner/Gate] Batch: 7‚úÖ 3‚è∏Ô∏è
[Learner/Gate] üíæ Saved (100 trees)
```

**Waiting States** (with reasons):
```
[Actor/PRO] ‚è∏Ô∏è No gaze (faces=2)
[Actor/PRO] ‚è∏Ô∏è Low IIE: Œº=0.42<0.5
[Actor/PRO] ‚è∏Ô∏è Unstable: œÉ¬≤=0.12‚â•0.1
```

**Gatekeeper Rejection** (Phase 3):
```
[Actor/PRO] üö´ Scene not opportune (8s ago)
```

**Always-On State Changes**:
```
[Actor/AO] ‚è∏Ô∏è No faces 120s ‚Üí stopping
[Actor/AO] ‚úÖ Stopped

[Actor/AO] ‚ñ∂Ô∏è Faces detected ‚Üí starting
[Actor/AO] ‚úÖ Started
```

**Self-Adaptor Activity**:
```
[Actor/SA] üîÑ ao_yawn
```

**Errors**:
```
[Actor/PRO] ‚ùå RPC: Connection refused
[Learner] ‚ùå Q-save: Permission denied
[Learner/Gate] ‚ùå Predict: Model file corrupt
```

**Shutdown**:
```
[Actor] üõë Shutting down...
[Actor] ‚úÖ Shutdown complete

[Learner] üõë Shutting down...
[Learner] üíæ Saved: Q=42, Gate=38
[Learner] ‚úÖ Shutdown complete
```
---

## Architecture

### Module Interaction Flow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                EMBODIED BEHAVIOUR MODULE                ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ IIE Monitor  ‚îÇ  ‚îÇContext Mon.  ‚îÇ  ‚îÇ Info Monitor ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ (Intention)  ‚îÇ  ‚îÇ (Calm/Lively)‚îÇ  ‚îÇ (Faces/Gaze) ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îÇ                           ‚îÇ                             ‚îÇ
‚îÇ                    Shared State                         ‚îÇ
‚îÇ              (IIE_mean, IIE_var, ctx,                   ‚îÇ
‚îÇ               num_faces, num_mutual_gaze)               ‚îÇ
‚îÇ                         ‚îÇ                               ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ         ‚îÇ                                ‚îÇ              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ  ‚îÇ  Proactive  ‚îÇ              ‚îÇ Self-Adaptor   ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ   Thread    ‚îÇ              ‚îÇ    Thread      ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ             ‚îÇ              ‚îÇ                ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Load Q    ‚îÇ              ‚îÇ ‚Ä¢ ao_yawn      ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Select    ‚îÇ              ‚îÇ ‚Ä¢ ao_cough     ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Execute   ‚îÇ              ‚îÇ ‚Ä¢ ao_look      ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Measure   ‚îÇ              ‚îÇ   (periodic)   ‚îÇ        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îÇ         ‚îÇ                                               ‚îÇ
‚îÇ         ‚îÇ Experience Bottle                             ‚îÇ
‚îÇ         ‚îÇ (13 fields)                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
          ‚îÇ YARP Port: /alwayson/embodiedbehaviour/experiences:o
          ‚îÇ
          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                       LEARNING MODULE                           ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ            Experience Processing Pipeline                  ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ         ‚îÇ                                                       ‚îÇ
‚îÇ         ‚ñº                                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                       ‚îÇ
‚îÇ  ‚îÇ 1. PREDICT  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Log Pred vs  ‚îÇ                       ‚îÇ
‚îÇ  ‚îÇ (ML Models) ‚îÇ         ‚îÇ   Actual     ‚îÇ                       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                       ‚îÇ
‚îÇ         ‚îÇ                                                       ‚îÇ
‚îÇ         ‚ñº                                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                       ‚îÇ
‚îÇ  ‚îÇ 2. COMPUTE  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Q-Learning   ‚îÇ                       ‚îÇ
‚îÇ  ‚îÇ   REWARD    ‚îÇ         ‚îÇ    Update    ‚îÇ                       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                       ‚îÇ
‚îÇ                                 ‚îÇ                               ‚îÇ
‚îÇ                                 ‚ñº                               ‚îÇ
‚îÇ                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                       ‚îÇ
‚îÇ                          ‚îÇ Save Q-table ‚îÇ                       ‚îÇ
‚îÇ                          ‚îÇ   (JSON)     ‚îÇ                       ‚îÇ
‚îÇ                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                       ‚îÇ
‚îÇ         ‚îÇ                                                       ‚îÇ
‚îÇ         ‚ñº                                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                       ‚îÇ
‚îÇ  ‚îÇ 3. TRAIN ML ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Buffer (4x)  ‚îÇ                       ‚îÇ
‚îÇ  ‚îÇ   MODELS    ‚îÇ         ‚îÇ Gatekeeper   ‚îÇ                       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Embodied Behaviour Module

### Overview
The actor module that perceives the environment, makes decisions, and executes actions.

### Thread Architecture (6 Threads)

#### 1. IIE Monitor Thread
**Purpose**: Tracks interaction intention estimation

**Input Port**: `/alwayson/embodiedbehaviour/iie:i`

**What it does**:
- Reads IIE distributions for all detected faces
- Each face has: `[mean, variance]`
- Selects the person with **highest mean** (most engaged)
- Updates shared state: `IIE_mean`, `IIE_var`

**Update Rate**: 20 Hz (every 0.05s)

**Example Bottle**:
```
((0 (faceID mean variance std)) (1 (faceID mean variance std)) ...)
```

#### 2. Context Monitor Thread
**Purpose**: Tracks environmental context classification

**Input Port**: `/alwayson/embodiedbehaviour/context:i`

**What it does**:
- Reads context label from STM
- `0` = Calm environment
- `1` = Lively environment
- `-1` = Uncertain
- Updates shared state: `ctx`

**Update Rate**: 10 Hz (every 0.1s)

**Example Bottle**:
```
(episode_id chunk_id label)
‚Üí (42 5 1) means episode 42, chunk 5, lively context
```

#### 3. Info Monitor Thread
**Purpose**: Tracks face count and mutual gaze

**Input Port**: `/alwayson/embodiedbehaviour/info:i`

**What it does**:
- Parses STM info bottle
- Extracts `Faces` and `MutualGaze` counts
- Updates shared state: `num_faces`, `num_mutual_gaze`
- Updates `last_faces_seen_time` (for always-on control)

**Update Rate**: 2 Hz (every 0.5s)

**Bottle Format**:
```
[
  (Time t_vision t_audio t_delta)
  (timestamp Faces n People n Light val Motion val MutualGaze n)
  (timestamp Audio (right left))
]
```

**Parsing Logic**:
```python
vision_list = bottle.get(1).asList()
# Skip timestamp, parse key-value pairs
i = 1
while i < vision_list.size() - 1:
    key = vision_list.get(i).toString()     # "Faces", "MutualGaze", etc.
    value = vision_list.get(i + 1)          # numeric value
    i += 2
```

#### 4. Always-On Monitor Thread
**Purpose**: Auto-stop/start system based on face presence

**What it does**:
- Monitors `num_faces` continuously
- **Stop condition**: No faces for 120 seconds
  - Executes: `echo "exe ao_stop" | yarp rpc /interactionInterface`
  - Sets `alwayson_active = False`
- **Start condition**: Faces detected while stopped
  - Executes: `echo "exe ao_start" | yarp rpc /interactionInterface`
  - Sets `alwayson_active = True`

**Update Rate**: 1 Hz (every 1.0s)

**State Variables**:
```python
alwayson_active: bool          # Current state (on/off)
last_faces_seen_time: float    # Timestamp of last face detection
NO_FACES_TIMEOUT: 120.0        # 2 minutes
```

#### 5. Proactive Thread (Learning Actions)
**Purpose**: Execute learned social behaviors

**Output Port**: `/alwayson/embodiedbehaviour/experiences:o`

**Available Actions**:
```python
PROACTIVE_ACTIONS = [
    "ao_greet",           # Greeting gesture
    "ao_coffee_break",    # Suggest break
    "ao_curious_lean_in"  # Lean forward with interest
]
```

**Decision Loop**:

```
1. Check if always-on is active
   ‚Üí Skip cycle if stopped

2. Capture pre-state snapshot (blocking 3s window)
   ‚Üí Collects 30 samples over 3.0 seconds at 0.1s intervals
   ‚Üí Averages to reduce noise

3. Check presence conditions:
   ‚úì num_faces > 0
   ‚úì num_mutual_gaze > 0

4. Check intention thresholds:
   ‚úì IIE_mean ‚â• 0.5 (THRESH_MEAN)
   ‚úì IIE_var < 0.1 (THRESH_VAR)

5. Load latest Q-table from disk
   ‚Üí Gets most recent learned values

6. Select action (epsilon-greedy):
   ‚Ä¢ 80% of time (initially): Exploit best Q-value
   ‚Ä¢ 20% of time: Explore random action
   ‚Ä¢ Epsilon decays: 0.8 ‚Üí 0.2 over time

7. Set proactive_active flag (signals self-adaptor to abort)

8. Execute action via subprocess RPC:
   subprocess.run('echo "exe {action}" | yarp rpc /interactionInterface')
   ‚Üí 5-second timeout, fire-and-forget pattern

9. Wait 3.0 seconds for action + reaction
   ‚Üí Wait allows action execution + human reaction to manifest

10. Capture post-state snapshot (blocking 3s window)
    ‚Üí Collects 30 samples over 3.0 seconds at 0.1s intervals
    ‚Üí Averages to measure outcome

11. Send experience to Learning module (13 fields)

12. Log to CSV

13. Decay epsilon: Œµ = max(0.2, Œµ √ó 0.957603)

14. Clear proactive_active flag

15. Cooldown 5 seconds
```

**Blocking Windowed Snapshot** (On-Demand Collection with Averaging):
```python
def _windowed_snapshot(duration=3.0, step=0.1):
    """Collect and average state over time window (blocking)
    
    Args:
        duration: Time window in seconds (default 3.0)
        step: Sampling interval in seconds (default 0.1)
        
    Returns:
        dict: Averaged state over ~30 samples
    """
    samples = []
    t0 = time.time()
    
    # Collect samples over duration
    while time.time() - t0 < duration and self.running:
        samples.append(self._get_state_snapshot())
        time.sleep(step)
    
    # Fallback to single snapshot if no samples
    if not samples:
        return self._get_state_snapshot()
    
    # Average numeric fields
    avg_mean = sum(s['IIE_mean'] for s in samples) / len(samples)
    avg_var = sum(s['IIE_var'] for s in samples) / len(samples)
    
    # Mode for categorical (most common context)
    ctx_mode = max(set(s['ctx'] for s in samples), 
                   key=[s['ctx'] for s in samples].count)
    
    # Round counts to nearest integer
    avg_faces = round(sum(s['num_faces'] for s in samples) / len(samples))
    avg_gaze = round(sum(s['num_mutual_gaze'] for s in samples) / len(samples))
    
    return {
        'IIE_mean': avg_mean,
        'IIE_var': avg_var,
        'ctx': ctx_mode,
        'num_faces': avg_faces,
        'num_mutual_gaze': avg_gaze
    }
```

**Epsilon-Greedy Selection**:
```python
def _select_action_epsilon_greedy(state_key):
    Q_values = Q[state_key]  # e.g., Q["CTX1"]
    
    if random() < epsilon:
        return random_choice(PROACTIVE_ACTIONS)  # Explore
    else:
        return argmax(Q_values)                  # Exploit
```

**Experience Bottle Format** (13 fields):
```
0:  timestamp (float64)
1:  action (string)
2:  pre_ctx (int8)
3:  pre_IIE_mean (float64)
4:  pre_IIE_var (float64)
5:  post_ctx (int8)
6:  post_IIE_mean (float64)
7:  post_IIE_var (float64)
8:  q_value (float64)
9:  pre_num_faces (int32)
10: pre_num_mutual_gaze (int32)
11: post_num_faces (int32)
12: post_num_mutual_gaze (int32)
```

#### 6. Self-Adaptor Thread (Background Behaviors)
**Purpose**: Periodic natural behaviors (non-learned)

**Available Actions**:
```python
SELF_ADAPTORS = [
    "ao_look_around",  # Look around environment
    "ao_yawn",         # Yawn gesture
    "ao_cough"         # Cough gesture
]
```

**Behavior**:
- **Random timing**: Each cycle uses random period between 60-120 seconds (1-2 minutes)
- **Always-on dependent**: Only executes when always-on is active
- **Proactive priority**: Aborts cycle if proactive action starts executing
  - Monitors proactive execution during sleep period
  - Aborts only after proactive passes thresholds and selects action
  - Waits for proactive to complete, then restarts with new random period
- **Random selection**: No learning involved
- **Purpose**: Make robot seem alive and natural during interactions

**Key Difference from Proactive**:
- ‚úó No threshold checks (executes regardless of IIE state)
- ‚úó No Q-learning
- ‚úó No epsilon-greedy
- ‚úó No experience sent to Learning
- ‚úì Respects always-on state (only when active)
- ‚úì Yields to proactive actions (priority system)
- ‚úì Random timing (60-120s per cycle)

---

## Learning Module

### Overview
The critic module that evaluates actions and improves the policy through Q-learning and ML models.

### Core Components

#### 1. Q-Learning (Value Function)

**Algorithm**: Temporal Difference (TD) Q-Learning

**Q-Table Structure**:
```json
{
  "CTX0": {  // Calm context
    "ao_greet": 0.23,
    "ao_coffee_break": 0.15,
    "ao_curious_lean_in": 0.41
  },
  "CTX1": {  // Lively context
    "ao_greet": 0.52,
    "ao_coffee_break": 0.38,
    "ao_curious_lean_in": 0.35
  }
}
```

**TD Update Rule**:
```python
Q(s, a) ‚Üê Q(s, a) + Œ± [r + Œ≥ ¬∑ max Q(s', a') - Q(s, a)]
                      ‚Üë       ‚Üë           ‚Üë
                   learn   immediate   best future
                   rate    reward      value
```

**Parameters**:
- `Œ± = 0.30`: Learning rate (how much to update)
- `Œ≥ = 0.92`: Discount factor (how much future matters)

**Update Process**:
```python
def _update_q(exp, reward):
    pre_state = f"CTX{exp.pre_ctx}"
    post_state = f"CTX{exp.post_ctx}"
    
    old_q = Q[pre_state][exp.action]
    max_next_q = max(Q[post_state].values())
    
    td_target = reward + 0.92 * max_next_q
    td_error = td_target - old_q
    new_q = old_q + 0.30 * td_error
    
    Q[pre_state][exp.action] = new_q
    return old_q, new_q, td_error
```

**Example Update**:
```
State: CTX1 (Lively)
Action: ao_greet
Old Q: 0.32

Reward received: +0.165
Best future Q (CTX1): 0.45

TD target: 0.165 + 0.92 √ó 0.45 = 0.579
TD error: 0.579 - 0.32 = 0.259
New Q: 0.32 + 0.30 √ó 0.259 = 0.398

Result: Q(CTX1, greet) updated 0.32 ‚Üí 0.40 ‚úì
```

**Persistence**:
- Saved after **every update** (atomic write)
- Format: JSON with temp+rename for safety
- Loaded by Embodied Behaviour before each action

#### 2. Reward Function

**Components** (4 terms + clipping):

```python
reward = (W_DELTA √ó delta_mean) + 
         (W_VAR √ó var_reduction) + 
         (W_LEVEL √ó level_term) - 
         action_cost

# Clip to [-1.0, 1.0] for stability
reward = max(-1.0, min(1.0, reward))
```

**1. Delta Term** (`W_DELTA = 1.0`):
```python
delta_mean = post_IIE_mean - pre_IIE_mean
if abs(delta_mean) < DELTA_EPS:  # 0.05 dead zone
    delta_mean = 0.0
```
- **Purpose**: Reward increasing engagement
- **Dead zone**: Filters noise ¬±0.05
- **Example**: 0.65 ‚Üí 0.75 gives +0.10

**2. Variance Term** (`W_VAR = 0.5`):
```python
var_reduction = pre_IIE_var - post_IIE_var
if abs(var_reduction) < VAR_EPS:  # 0.02 dead zone
    var_reduction = 0.0
```
- **Purpose**: Reward making interaction more predictable
- **Dead zone**: Filters noise ¬±0.02
- **Example**: 0.08 ‚Üí 0.05 gives +0.03 (reduction)

**3. Level Term** (`W_LEVEL = 0.5`):
```python
level_term = post_IIE_mean - THRESH_MEAN  # 0.5 threshold
if post_IIE_mean < THRESH_MEAN:
    level_term *= 2.0  # Double penalty!
```
- **Purpose**: Maintain engagement above threshold
- **Positive**: Comfortably above 0.5
- **Negative**: Below 0.5 (2√ó penalty)
- **Example**: 
  - IIE = 0.67: level_term = +0.17 (good!)
  - IIE = 0.45: level_term = -0.10 (penalty √ó2)

**4. Action Cost**:
```python
ACTION_COSTS = {
    "ao_greet": 0.08,
    "ao_coffee_break": 0.10,
    "ao_curious_lean_in": 0.06,
}
```
- **Purpose**: Small penalty for energy expenditure
- Prevents action spam

**Complete Example**:
```python
# Scenario: Successful greeting in lively context
pre_IIE_mean:  0.62
post_IIE_mean: 0.78
pre_IIE_var:   0.08
post_IIE_var:  0.06
action:        "ao_greet"

# Compute components:
delta_mean = 0.78 - 0.62 = 0.16 (‚úì above dead zone)
var_reduction = 0.08 - 0.06 = 0.02 (‚úì just at dead zone)
level_term = 0.78 - 0.5 = 0.28 (‚úì well above threshold)
action_cost = 0.08

# Final reward:
reward = 1.0√ó0.16 + 0.5√ó0.02 + 0.5√ó0.28 - 0.08
       = 0.16 + 0.01 + 0.14 - 0.08
       = +0.23 ‚úì Strong positive!

# After clipping (already within [-1.0, 1.0]):
reward = 0.23 (no change)
```

**Reward Philosophy**:
- **Delta**: Reward improvement
- **Variance**: Reward stability
- **Level**: Maintain high engagement (don't break interaction!)
- **Cost**: Discourage excessive actions
- **Clipping**: Bound to [-1.0, 1.0] for numerical stability (prevents outliers)

#### 3. Gatekeeper Model (Scene Discriminator)

**Purpose**: Learn to recognize pre-conditions that lead to engagement improvement

**Philosophy**: 
- **Learns from RAW OUTCOMES**: Independent of Q-learning reward function
- **Binary Classification**: Did engagement actually increase after this action?
- **Physical Reality**: Compares post-IIE vs pre-IIE directly (no weights, no penalties)

**Model**: Binary Classifier
```python
GradientBoostingClassifier(
    n_estimators=100‚Üí200,  # Grows over time
    max_depth=3,
    learning_rate=0.1,
    warm_start=True
)
```

**Input Features** (6D - **PRE-STATE ONLY**):
```python
[
    pre_IIE_mean,           # Interaction intention (before action)
    pre_IIE_var,            # Intention stability (before action)  
    pre_ctx,                # Context (before action)
    pre_num_faces,          # Audience size (before action)
    pre_num_mutual_gaze,    # Attention (before action)
    time_delta              # Time since last action (seconds)
]
# These are the ONLY features used for prediction
# The model decides based on what it sees BEFORE acting
```

**Critical Design**: Uses **only** pre-action values + timing. The model must learn to recognize good opportunities based solely on what it sees *before* acting.

**Output**: Binary Label
- `1` (YES): "This scene historically leads to positive outcomes - ACT"
- `0` (NO): "This scene historically leads to negative outcomes - WAIT"

**Training Label Generation** (Raw Outcome Learning):
```python
# Calculate raw improvement (independent of Q-learning reward)
raw_delta = post_IIE_mean - pre_IIE_mean

# Label based on physical reality (did engagement actually increase?)
if raw_delta > 0.02:  # Small positive threshold
    label = 1  # YES: These pre-conditions led to improvement
else:
    label = 0  # NO: These pre-conditions did not improve engagement

# Key: This is INDEPENDENT of the Q-learning reward function
# Q-learning uses weighted components (delta, variance, level, cost)
# Gatekeeper uses simple post - pre comparison
```

**Training**:
- Buffer size: 4 samples
- When buffer full: train on batch
- Incremental: adds 5 trees per training
- Max trees: 200
- Saves after each training

**Scene Clustering Logic**:
The gatekeeper learns patterns by observing raw outcomes:
- **"YES" Cluster**: Pre-conditions that historically led to +0.02 IIE improvement
  - Example: High IIE (0.6) + 2 faces + 15s elapsed ‚Üí Improved to 0.65
- **"NO" Cluster**: Pre-conditions that did not lead to improvement
  - Example: Low IIE (0.45) + 0 gaze + 5s elapsed ‚Üí Stayed at 0.45 or dropped

**Key Insight**: The model discovers patterns by comparing what happened (post) to what was (pre), not by trusting a reward function's weighted opinion.

**Prediction Interface**:
```python
def predict_decision(pre_IIE_mean, pre_IIE_var, pre_ctx, 
                     pre_num_faces, pre_num_mutual_gaze, time_delta):
    """Returns True (ACT) or False (WAIT)
    
    Args:
        pre_IIE_mean: Interaction intention (0-1 scale)
        pre_IIE_var: Intention variance/stability
        pre_ctx: Context classification (0=Calm, 1=Lively)
        pre_num_faces: Number of faces detected
        pre_num_mutual_gaze: Number of people with mutual gaze
        time_delta: Seconds since last action
    """
    features = [pre_IIE_mean, pre_IIE_var, pre_ctx, 
                pre_num_faces, pre_num_mutual_gaze, time_delta]
    prediction = gate_model.predict([features])[0]
    return bool(prediction == 1)
```

**Usage in Decision-Making** (Future Integration):
```python
# In embodiedBehaviour.py proactive loop:
if gatekeeper.predict_decision(pre_state, time_since_last_action):
    # Scene matches "winning" cluster ‚Üí Execute action
    execute_action()
else:
    # Scene matches "losing" cluster ‚Üí Wait
    wait_for_better_opportunity()
```

#### 4. Experience Processing Pipeline

**Streamlined Order** (Gatekeeper Architecture):

```python
def _process_experience(exp):
    # 1. CALCULATE TIME DELTA
    time_delta = exp.timestamp - last_exp_timestamp
    
    # WHY: Timing is a critical feature for opportunity recognition
    
    # 2. Q-LEARNING UPDATE
    reward = _compute_reward(exp)
    old_q, new_q, td_error = _update_q(exp, reward)
    _save_qtable()  # Save immediately!
    
    # WHY: Embodied Behaviour loads Q-table before next action
    
    # 3. TRAIN GATEKEEPER (Scene Discriminator)
    label = 1 if reward > 0.05 else 0  # YES/NO scene
    features = _encode_gate_features(exp, time_delta)  # 6D pre-state
    _train_gate_model(features, label, reward)
    
    # WHY: Learn to recognize winning vs losing scenes based on
    #      pre-action conditions and timing
    
    # 4. UPDATE TIMESTAMP
    last_exp_timestamp = exp.timestamp
    
    # WHY: Enable time delta calculation for next experience
```
---

## Communication Protocol

### Port Connections

```
STM ‚Üí Embodied Behaviour:
  /alwayson/stm/info:o ‚Üí /alwayson/embodiedbehaviour/info:i
  /alwayson/stm/context:o ‚Üí /alwayson/embodiedbehaviour/context:i
  
IIE Estimator ‚Üí Embodied Behaviour:
  /iie/estimator:o ‚Üí /alwayson/embodiedbehaviour/iie:i

Embodied Behaviour ‚Üí Learning:
  /alwayson/embodiedbehaviour/experiences:o ‚Üí /alwayson/learning/experiences:i

Robot Interface:
  RPC: /interactionInterface (for action execution)
```

### Experience Bottle (13 Fields)

**Sent by**: Embodied Behaviour  
**Received by**: Learning  
**Frequency**: After each proactive action (~10-20 seconds)

```python
bottle.addFloat64(timestamp)              # 0: When action started
bottle.addString(action)                  # 1: "ao_greet", etc.
bottle.addInt8(pre_ctx)                   # 2: Context before (-1/0/1)
bottle.addFloat64(pre_IIE_mean)           # 3: Engagement before
bottle.addFloat64(pre_IIE_var)            # 4: Variance before
bottle.addInt8(post_ctx)                  # 5: Context after
bottle.addFloat64(post_IIE_mean)          # 6: Engagement after
bottle.addFloat64(post_IIE_var)           # 7: Variance after
bottle.addFloat64(q_value)                # 8: Q-value used
bottle.addInt32(pre_num_faces)            # 9: Faces before
bottle.addInt32(pre_num_mutual_gaze)      # 10: Gaze before
bottle.addInt32(post_num_faces)           # 11: Faces after
bottle.addInt32(post_num_mutual_gaze)     # 12: Gaze after
```

---

## Implementation Notes

### Current State (Active Features)

**Embodied Behaviour Module**:
- ‚úÖ 6-thread architecture fully operational
- ‚úÖ Blocking windowed snapshots (3s duration, 0.1s steps)
- ‚úÖ Subprocess-based RPC execution
- ‚úÖ Self-adaptor priority system (60-120s random periods)
- ‚úÖ Always-on auto-stop/start mechanism
- ‚úÖ Proactive actions with Q-learning
- ‚úÖ Thread-safe state management

**Learning Module**:
- ‚úÖ Q-learning with TD(0) updates
- ‚úÖ Multi-component reward function
- ‚úÖ Gatekeeper model training (buffer size: 4)
- ‚úÖ JSON Q-table persistence
- ‚úÖ PKL model persistence
- ‚úÖ CSV logging for both Q-learning and gatekeeper

### Phase 3 Feature (Disabled)

**Gatekeeper Integration in embodiedBehaviour.py**:
- Status: **Commented out** (lines ~461-487)
- Reason: Requires 200+ training samples before activation
- Location: Proactive thread, after threshold checks
- Activation steps:
  1. Uncomment gatekeeper check block
  2. Uncomment `_check_gatekeeper()` method
  3. Uncomment `self.last_action_time` tracking in `__init__`
  4. Add `import pickle, numpy` at top

**When to activate**:
- After collecting 200+ experiences (50 full buffer cycles)
- When `gate_classifier.pkl` file exists and is trained
- To add scene discrimination layer before action execution

---

## Configuration & Parameters

### Embodied Behaviour

**Action Thresholds**:
```python
THRESH_MEAN = 0.5     # Minimum IIE to act
THRESH_VAR = 0.1      # Maximum variance to act
```

**Timing**:
```python
WAIT_AFTER_ACTION = 3.0          # Seconds for action execution + human reaction
COOLDOWN = 5.0                   # Seconds between proactive actions (hardcoded)
NO_FACES_TIMEOUT = 120.0         # Always-on: stop after (2 min)
# Self-adaptor: random period between 60-120s per cycle (1-2 minutes)
# (Context-dependent timing removed in favor of random periods)
```

**Exploration**:
```python
EPSILON = 0.8            # Initial exploration rate
EPSILON_MIN = 0.2        # Minimum exploration rate
EPSILON_DECAY = 0.957603 # Decay per action
```

**Windowed Snapshots** (Blocking On-Demand Collection):
```python
# _windowed_snapshot(duration=3.0, step=0.1)
# Collects ~30 samples over 3 seconds for noise filtering
# Pre-state: 3s blocking collection
# Post-state: 3s blocking collection
# Total measurement time: 6s per action cycle
```

**Priority System**:
```python
# Proactive thread signals execution state via proactive_active flag
# Self-adaptor monitors flag during sleep period
# Aborts cycle only after proactive selects action (not during threshold checks)
# Restarts with new random period after proactive completes
```

### Learning

**Q-Learning**:
```python
ALPHA = 0.30    # Learning rate
GAMMA = 0.92    # Discount factor
```

**Reward Weights**:
```python
W_DELTA = 1.0   # IIE change weight
W_VAR = 0.5     # Variance reduction weight
W_LEVEL = 0.5   # Engagement level weight
THRESH_MEAN = 0.5  # Threshold for level term
```

**Dead Zones** (Noise Filtering):
```python
DELTA_EPS = 0.05  # Minimum IIE change (¬±0.05)
VAR_EPS = 0.02    # Minimum variance change (¬±0.02)
```

**Gatekeeper Model**:
```python
BUFFER_SIZE = 4                 # Training batch size
GATE_MAX_DEPTH = 3              # Tree depth
GATE_N_ESTIMATORS = 100         # Initial trees
GATE_MAX_ESTIMATORS = 200       # Maximum trees
GATE_LEARNING_RATE = 0.1        # Boosting learning rate
REWARD_THRESHOLD = 0.05         # Minimum reward for "YES" label
```

**Action Costs**:
```python
ACTION_COSTS = {
    "ao_greet": 0.08,
    "ao_coffee_break": 0.10,
    "ao_curious_lean_in": 0.06,
}
```

---

## Data Logging

### Embodied Behaviour Logs

#### 1. Proactive Log (`proactive_log.csv`)
**Frequency**: Every proactive action  
**Fields** (14 columns):
```csv
timestamp, proactive_action,
pre_IIE_mean, pre_IIE_var, pre_ctx, pre_num_faces, pre_num_mutual_gaze,
post_IIE_mean, post_IIE_var, post_ctx, post_num_faces, post_num_mutual_gaze,
q_value, epsilon
```

**Purpose**: Track action selection and outcomes

#### 2. Self-Adaptor Log (`selfadaptor_log.csv`)
**Frequency**: Every self-adaptor action  
**Fields** (8 columns):
```csv
timestamp, self_adaptor_action,
pre_IIE_mean, pre_IIE_var, pre_ctx,
post_IIE_mean, post_IIE_var, post_ctx
```

**Purpose**: Track background behaviors

### Learning Logs

#### 1. Q-Learning Log (`qlearning_log.csv`)
**Frequency**: Every Q-update  
**Fields** (14 columns):
```csv
timestamp, proactive_action, pre_state, post_state,
pre_IIE_mean, post_IIE_mean, delta_mean,
pre_IIE_var, post_IIE_var, var_reduction,
reward, old_q, new_q, td_error
```

**Purpose**: Track reward and Q-value evolution

**Key Metrics**:
- `reward`: Total reward received
- `td_error`: Prediction error (how surprising was outcome?)
- `new_q - old_q`: How much Q-value changed

#### 2. Gatekeeper Training Log (`gate_training_log.csv`)
**Frequency**: Every experience  
**Fields** (12 columns):
```csv
timestamp, pre_IIE_mean, pre_IIE_var, pre_ctx,
pre_num_faces, pre_num_mutual_gaze, time_delta,
post_IIE_mean, raw_delta, label, reward_ref, gate_count
```

**Purpose**: Track scene discrimination training

**Key Metrics**:
- `label`: 1 (YES - improved) or 0 (NO - no improvement)
- `raw_delta`: post_IIE - pre_IIE (the actual outcome)
- `post_IIE_mean`: Engagement level after action
- `reward_ref`: Q-learning reward (logged for reference, NOT used in gatekeeper training)
- `gate_count`: Training sample number

**Analysis**:
- Track YES/NO balance (should stabilize around natural improvement rate)
- Identify which pre-conditions lead to raw IIE increases
- Monitor raw_delta distribution (how often does engagement actually improve?)
- Compare `label` (raw outcome) vs `reward_ref` (Q-learning) to see divergence

---

## Reward Function Design

### Design Philosophy

**Goal**: Learn to maintain high engagement while avoiding interaction breakdowns

**Challenges**:
1. **Noise**: IIE measurements flicker ¬±0.03
2. **Threshold**: Must stay above 0.5 to act
3. **Sparse signal**: Changes are often small

**Solutions**:

#### 1. Dead Zones (Noise Filtering)
```python
if abs(delta_mean) < 0.05:
    delta_mean = 0.0  # Ignore tiny changes
```

**Benefit**: Random jitter doesn't create false rewards

#### 2. Windowed Averaging (Noise Filtering)
```python
pre = average_over_3_seconds()   # 30 samples
post = average_over_3_seconds()  # 30 samples
```

**Benefit**: Pre/post measurements are highly stable (~30√ó noise reduction)

#### 3. Level Term (Threshold Awareness)
```python
level_term = post_IIE_mean - 0.5
if post_IIE_mean < 0.5:
    level_term *= 2.0  # Penalty!
```

**Benefit**: 
- Don't break engagement (strong penalty)
- Maintain high IIE (mild reward)

### Reward Examples

**Scenario 1: Maintain engagement**
```python
Pre:  IIE = 0.65, var = 0.08  # From 3s windowed snapshot average
Post: IIE = 0.67, var = 0.06  # From 3s windowed snapshot average
Action: ao_greet

delta_mean = 0.02 ‚Üí 0.0 (dead zone)
var_reduction = 0.02 ‚Üí 0.02 (at threshold)
level_term = 0.67 - 0.5 = 0.17

reward = 0.0 + 0.5√ó0.02 + 0.5√ó0.17 - 0.08
       = 0.0 + 0.01 + 0.085 - 0.08
       = +0.015 (slightly positive!)
# Clipped: 0.015 (within bounds)
```
‚úì Maintaining high engagement is rewarded

**Scenario 2: Break engagement**
```python
Pre:  IIE = 0.55, var = 0.07
Post: IIE = 0.45, var = 0.09
Action: ao_coffee_break

delta_mean = -0.10
var_reduction = -0.02
level_term = (0.45 - 0.5) √ó 2.0 = -0.10 (√ó2 penalty!)

reward = 1.0√ó(-0.10) + 0.5√ó(-0.02) + 0.5√ó(-0.10) - 0.10
       = -0.10 - 0.01 - 0.05 - 0.10
       = -0.26 (strong penalty!)
# Clipped: -0.26 (within bounds)
```
‚úó Breaking threshold is heavily penalized

**Scenario 3: Boost engagement**
```python
Pre:  IIE = 0.60, var = 0.08
Post: IIE = 0.78, var = 0.05
Action: ao_curious_lean_in

delta_mean = 0.18
var_reduction = 0.03
level_term = 0.78 - 0.5 = 0.28

reward = 1.0√ó0.18 + 0.5√ó0.03 + 0.5√ó0.28 - 0.06
       = 0.18 + 0.015 + 0.14 - 0.06
       = +0.275 (excellent!)
# Clipped: 0.275 (within bounds)
```
‚úì Large improvement + high level = big reward

---

## Learning Progress & Convergence

### Phases of Learning

#### Phase 1: Exploration (Episodes 1-50)
**Epsilon**: 0.8 ‚Üí 0.6

**Behavior**:
- 80% random actions (exploring)
- 20% best known actions (exploiting)
- Building initial Q-table
- High variance in outcomes

**Q-Table Example** (after 20 actions):
```json
{
  "CTX0": {
    "ao_greet": 0.12,
    "ao_coffee_break": -0.05,  // Doesn't work well in calm
    "ao_curious_lean_in": 0.28
  },
  "CTX1": {
    "ao_greet": 0.35,
    "ao_coffee_break": 0.18,
    "ao_curious_lean_in": 0.22
  }
}
```

**ML Models**:
- IIE Model: Not trained yet (need 10 samples)
- Context Model: Not trained yet
- Prediction error: N/A

#### Phase 2: Refinement (Episodes 51-150)
**Epsilon**: 0.6 ‚Üí 0.3

**Behavior**:
- 40% random (still exploring)
- 60% best known (mostly exploiting)
- Q-values converging
- Patterns emerging

**Q-Table Example** (after 100 actions):
```json
{
  "CTX0": {
    "ao_greet": 0.23,
    "ao_coffee_break": 0.15,
    "ao_curious_lean_in": 0.41  // Clear winner in calm
  },
  "CTX1": {
    "ao_greet": 0.52,           // Clear winner in lively
    "ao_coffee_break": 0.38,
    "ao_curious_lean_in": 0.35
  }
}
```

**Gatekeeper Model**:
- Trained 10+ times, 100+ trees
- Learning scene patterns: "High IIE + 15s wait ‚Üí YES"
- Scene discrimination emerging

#### Phase 3: Convergence (Episodes 150+)
**Epsilon**: 0.3 ‚Üí 0.2

**Behavior**:
- 20% random (minimal exploration)
- 80% best known (mostly exploiting)
- Q-values stable
- Confident action selection

**Q-Table Example** (after 300 actions):
```json
{
  "CTX0": {
    "ao_greet": 0.25,
    "ao_coffee_break": 0.18,
    "ao_curious_lean_in": 0.47  // Stable best
  },
  "CTX1": {
    "ao_greet": 0.56,           // Stable best
    "ao_coffee_break": 0.41,
    "ao_curious_lean_in": 0.38
  }
}
```

**Gatekeeper Model**:
- 200 trees, fully trained
- Clear scene clustering: YES (60%) vs NO (40%)
- Recognizes winning patterns:
  - **YES Cluster**: High IIE (>0.6) + Mutual gaze + Time elapsed (>12s)
  - **NO Cluster**: Low IIE (<0.4) OR Too soon (<8s) OR No attention

### Convergence Indicators

**Q-values stabilizing**:
```python
# Watch in qlearning_log.csv
td_error ‚Üí 0 (predictions match reality)
new_q - old_q ‚Üí 0 (no more updates)
```

**Gatekeeper raw outcome learning**:
```python
# Watch in gate_training_log.csv
YES scenes: Pre-conditions where raw_delta > 0.02 (actual improvement)
NO scenes: Pre-conditions where raw_delta ‚â§ 0.02 (no improvement)
Label distribution: Reflects natural improvement rate (varies by interaction quality)
raw_delta: Mean should be positive if actions are generally helpful
```

**Reward increasing**:
```python
# Watch in qlearning_log.csv
mean(reward) increasing over time
variance(reward) decreasing over time
```

**Epsilon decaying**:
```python
# Watch in proactive_log.csv
epsilon: 0.8 ‚Üí 0.6 ‚Üí 0.4 ‚Üí 0.2
```

### Expected Timeline

**Quick learning**: 20-30 actions
- Initial patterns emerge
- Q-values start differentiating

**Competent**: 50-100 actions
- Reliable action selection
- Context-appropriate behaviors

**Expert**: 150-300 actions
- Near-optimal policy
- Confident predictions

---

## Future Integration: Gatekeeper-Gated Actions

### Overview
After the gatekeeper model has collected sufficient training data (200+ experiences, ~Phase 3+), it can be integrated into the proactive action loop to add learned timing intelligence on top of hard-coded thresholds.

### Current Architecture (Phase 1-2)
```
Thresholds Met ‚Üí Load Q-Table ‚Üí Select Action ‚Üí Execute
   (IIE ‚â• 0.5, Var < 0.1, Faces > 0, Gaze > 0)
```

### Future Architecture (Phase 3+)
```
Thresholds Met ‚Üí Gatekeeper Check ‚Üí Load Q-Table ‚Üí Select Action ‚Üí Execute
   (Hard-coded)    (Learned timing)
```

### Integration Point
The gatekeeper will be queried **after** threshold checks but **before** action selection in the proactive thread.

**Cross-Process Solution**: Since `embodiedBehaviour.py` and `learning.py` run as separate YARP modules (different processes), they cannot directly share Python objects. The solution uses **disk-based model sharing**:

1. **Learning module** trains and saves `gate_classifier.pkl` to disk
2. **Embodied Behaviour module** loads the model file when making decisions
3. Both modules access the same file (Learning writes, Actor reads)

**Implementation**:

```python
# ============================================================
# TODO: GATEKEEPER INTEGRATION (PHASE 3+)
# ============================================================
# Uncomment this block after 200+ training samples collected
# Requires: pickle, numpy imports (see top of file)
# Requires: self.last_action_time tracking (see __init__)
# ============================================================

# Calculate time since last action
current_time = time.time()
time_delta = current_time - getattr(self, 'last_action_time', current_time - 10.0)

# Query disk-based gatekeeper model
should_act = self._check_gatekeeper(pre, time_delta)

if not should_act:
    print(f"[Proactive] üö´ Gatekeeper: Scene not opportune (wait {time_delta:.1f}s)")
    time.sleep(2.0)
    continue

print(f"[Proactive] ‚úÖ Gatekeeper: Scene opportune (proceed)")

# Update timestamp for next action
self.last_action_time = time.time()
# ============================================================
```

**Helper Method** (add to `embodiedBehaviour.py`):

```python
def _check_gatekeeper(self, pre, time_delta):
    """Load gatekeeper model from disk and predict 'Should I Act?'
    
    PHASE 3 INTEGRATION: Uncomment this method after 200+ training samples
    
    Args:
        pre: Pre-state snapshot dict
        time_delta: Time since last action (seconds)
    
    Returns:
        bool: True = ACT (scene opportune), False = WAIT (scene not opportune)
    """
    model_path = os.path.join(os.path.dirname(__file__), "gate_classifier.pkl")
    
    # If no model exists yet (early training), default to YES (allow exploration)
    if not os.path.exists(model_path):
        return True
    
    try:
        # Load the trained model from disk (read-only)
        with open(model_path, 'rb') as f:
            model = pickle.load(f)
        
        # Encode features: MUST MATCH learning.py _encode_gate_features() ORDER EXACTLY
        # [pre_IIE_mean, pre_IIE_var, pre_ctx, pre_num_faces, pre_num_mutual_gaze, time_delta]
        features = np.array([[
            pre['IIE_mean'],               # Interaction intention (pre)
            pre['IIE_var'],                # Intention stability (pre)
            float(pre['ctx']),             # Context (pre)
            float(pre['num_faces']),       # Audience size (pre)
            float(pre['num_mutual_gaze']), # Attention (pre)
            float(time_delta)              # Time since last action
        ]])
        
        # Predict: 1 = YES (opportune scene), 0 = NO (wait)
        prediction = model.predict(features)[0]
        return prediction == 1
    
    except Exception as e:
        print(f"[Gatekeeper] ‚ö†Ô∏è Prediction error: {e}")
        return True  # Default to ACT if model file is corrupt/busy
```

### Benefits
1. **Learned from Physical Reality**: Discovers what actually works (raw IIE changes)
   - Not biased by reward function weights or penalties
   - Learns: "These conditions historically led to +0.05 IIE improvement"
2. **Scene Pattern Recognition**: Identifies improvement-prone vs stagnant scenes
   - **YES Cluster**: Conditions that led to engagement increases
   - **NO Cluster**: Conditions that didn't improve engagement
3. **Reduced Wasted Actions**: Avoids acting in scenes that historically don't improve
4. **Independent Validation**: Can compare gatekeeper decisions to Q-learning to spot issues

### Training Requirements (Before Integration)
- **Minimum samples**: 200 experiences (buffer fills 50 times)
- **Phase**: Episode 100+ (Phase 3 convergence)
- **Gatekeeper convergence**: Clear YES/NO scene clustering visible in `gate_training_log.csv`
- **Label balance**: Stabilized around 60% YES, 40% NO

### Implementation Checklist

**Phase 1-2 (Collect Training Data)**:
- [ ] Run system with threshold-based actions only
- [ ] Collect 200+ experiences with diverse scenes
- [ ] Verify gatekeeper scene clustering in `gate_training_log.csv`
- [ ] Check label balance (should stabilize ~60% YES, 40% NO)
- [ ] Confirm `gate_classifier.pkl` exists and grows to 200 trees

**Phase 3 (Activate Gatekeeper)**:
- [ ] **Add imports** to `embodiedBehaviour.py`:
  ```python
  import pickle
  import numpy as np
  ```
- [ ] **Add timing tracker** to `__init__`:
  ```python
  self.last_action_time = time.time()
  ```
- [ ] **Uncomment `_check_gatekeeper()` method** (already in code, commented)
- [ ] **Uncomment integration block** in proactive loop (already in code, commented)
- [ ] **Test gated actions** in live interaction
- [ ] **Monitor rejection rate**: Expect 20-40% of opportunities rejected with "Scene not opportune"
- [ ] **Compare performance**: Track reward improvement with gatekeeper vs without

**Rollback Plan** (if gatekeeper performs poorly):
- [ ] Re-comment the integration block
- [ ] System reverts to threshold-based decisions
- [ ] Gatekeeper continues training in background (no impact on actions)

### Final Activation Steps

**Current System Status**: ‚úÖ **GREEN** - Code is clean, documented, and logically consistent. Ready to deploy.

**Current Mode**: Phase 1/2 (Data Collection)
- The gatekeeper logic in `embodiedBehaviour.py` is **commented out**
- System runs threshold-based actions only
- Gatekeeper trains passively in background (via `learning.py`)

**To Activate Phase 3 (AI Gating)**:

After collecting 200+ training samples and verifying gatekeeper convergence:

1. **Uncomment Line 56** in `embodiedBehaviour.py`:
   ```python
   self.last_action_time = time.time()
   ```
   *(Enables timing tracker for action intervals)*

2. **Uncomment Lines 407-438** in `embodiedBehaviour.py`:
   ```python
   # ============================================================
   # TODO: GATEKEEPER INTEGRATION (PHASE 3+)
   # ============================================================
   # [Full integration block with time_delta calculation and gatekeeper check]
   # ============================================================
   ```
   *(Enables gatekeeper decision-making in proactive loop)*

3. **Uncomment Lines 530-569** in `embodiedBehaviour.py`:
   ```python
   def _check_gatekeeper(self, pre, time_delta):
       """Load gatekeeper model from disk and predict 'Should I Act?'"""
       # [Full helper method implementation]
   ```
   *(Enables disk-based model loading and prediction)*

**Verification**:
- System will start logging: `"[Actor/PRO] ‚úÖ Scene opportune"` or `"[Actor/PRO] üö´ Scene not opportune (Xs ago)"`
- Expect 20-40% rejection rate in typical interaction scenarios
- Monitor `gate_training_log.csv` for decision patterns
- Console dashboard will show gatekeeper decisions in real-time

**Rollback** (if needed):
- Re-comment the three sections above
- System reverts to Phase 1/2 threshold-based decisions
- No data loss, gatekeeper continues training

### Why Wait Until Phase 3?
- **Phase 1-2**: Gatekeeper has insufficient data, predictions unreliable
- **Phase 3**: Model has seen diverse scenes, clusters formed, timing patterns learned
- **Risk of early integration**: Random rejections, disrupted exploration
- **Benefit of delayed integration**: Stable timing intelligence, proven patterns

### Technical Notes

**Cross-Process Architecture**:
- `embodiedBehaviour.py` and `learning.py` are separate YARP modules (different OS processes)
- Cannot share Python objects directly (no shared memory)
- Solution: File-based model sharing via `gate_classifier.pkl`

**File Access Pattern**:
- **Learning module**: Writes `gate_classifier.pkl` after every 10 training samples
- **Embodied Behaviour**: Reads `gate_classifier.pkl` before each decision
- **Concurrency**: Read-only access from Actor is safe (no write conflicts)
- **Failure mode**: If file is corrupted/busy, defaults to `True` (allow action)

**Feature Encoding Consistency**:
- **CRITICAL**: Both modules must encode features in **identical order**
- Learning module defines order in `_encode_gate_features()`:
  ```python
  [pre_IIE_mean,           # Interaction intention (pre)
   pre_IIE_var,            # Intention stability (pre)
   pre_ctx,                # Context (pre)
   pre_num_faces,          # Audience size (pre)
   pre_num_mutual_gaze,    # Attention (pre)
   time_delta]             # Time since last action
  ```
- Actor module must match exactly in `_check_gatekeeper()` (see helper method above)
- Mismatch will cause prediction errors

**Performance Impact**:
- Model loading: ~5-10ms per decision (negligible vs 9s action cycle)
- Prediction: ~1-2ms (GradientBoosting inference)
- Total overhead: <15ms per action (~0.2% of cycle time)

---

## Appendix: Mathematical Notation

### Q-Learning
```
Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ max Q(s',a') - Q(s,a)]
                      a'

Where:
  Q(s,a): Value of action a in state s
  Œ±: Learning rate (0.30)
  r: Reward received
  Œ≥: Discount factor (0.92)
  s': Next state
  max Q(s',a'): Best future value
     a'
```

### Reward Function
```
R_raw = W_Œî¬∑ŒîŒº + W_œÉ¬∑ŒîœÉ¬≤ + W_L¬∑L - C

R = clip(R_raw, -1.0, 1.0)  # Bounded for stability

Where:
  ŒîŒº = {Œº_post - Œº_pre  if |ŒîŒº| ‚â• 0.05
       {0                otherwise
  
  ŒîœÉ¬≤ = {œÉ¬≤_pre - œÉ¬≤_post  if |ŒîœÉ¬≤| ‚â• 0.02
        {0                  otherwise
  
  L = {(Œº_post - 0.5)      if Œº_post ‚â• 0.5
      {2(Œº_post - 0.5)      if Œº_post < 0.5
  
  C = action cost
  
  W_Œî = 1.0, W_œÉ = 0.5, W_L = 0.5
```

### Epsilon-Greedy
```
œÄ(s) = {argmax Q(s,a)    with probability 1-Œµ
       {random action     with probability Œµ
        a

Œµ_t = max(Œµ_min, Œµ_0 ¬∑ Œª^t)

Where:
  Œµ_0 = 0.8 (initial)
  Œµ_min = 0.2 (minimum)
  Œª = 0.957603 (decay rate)
  t = action count
```
