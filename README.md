# Embodied Behaviour & Learning Modules Documentation

## Table of Contents
1. [System Overview](#system-overview)
2. [Console Dashboard](#console-dashboard)
3. [Architecture](#architecture)
4. [Embodied Behaviour Module](#embodied-behaviour-module)
5. [Learning Module](#learning-module)
6. [Communication Protocol](#communication-protocol)
7. [Configuration & Parameters](#configuration--parameters)
8. [Data Logging](#data-logging)
9. [Reward Function Design](#reward-function-design)
10. [Learning Progress & Convergence](#learning-progress--convergence)

---

## System Overview

### Purpose
This system implements a **developmental reinforcement learning architecture** for social robot interaction. The robot learns which social behaviors (greetings, gestures, etc.) are most effective in different contexts through experience.

### Core Concept
- **Actor-Critic Architecture**: Two independent modules work together
  - **Embodied Behaviour** (Actor): Perceives, decides, and acts
  - **Learning** (Critic): Evaluates actions and improves decision-making

### Key Features
- **Real-time learning**: Updates after every interaction
- **Context-aware**: Adapts behavior to calm vs. lively environments
- **Noise-robust**: Filters out measurement jitter with dead zones and rolling window averaging (3s history, 60 samples at 20Hz)
- **Exploration-exploitation**: Balances trying new things vs. using known strategies
- **Always-on autonomy**: Auto-stops when no one is present, restarts when people return
- **Data freshness monitoring**: Detects and handles stale sensor data to prevent frozen state spam
- **High-performance**: Non-blocking state capture (0.0s latency), native YARP RPC, fire-and-forget action execution

### Performance Optimizations

**1. Rolling Window Buffers (Non-Blocking, Thread-Safe)**
- IIE Monitor continuously populates a 60-sample rolling buffer at 20Hz
- State snapshots read instantly from buffer (0.0s vs 3.0s blocking)
- Maintains 3-second noise filtering without wait time
- Pre/post state capture: **6.0s ‚Üí 0.0s** (instant)
- Thread-safe: `_state_lock` protects deque from concurrent append/iteration race conditions

**2. Native YARP RPC (Fire-and-Forget)**
- Uses native `yarp.RpcClient` for action execution
- Direct bottle communication to `/interactionInterface`
- Fire-and-forget pattern: no reply checking needed
- Action execution: **faster, more reliable**

**3. Data Freshness Monitoring**
- Detects stale/frozen sensor streams
- IIE timeout: 5.0s (expected 20Hz updates)
- Info timeout: 5.0s (expected 2Hz updates)
- Auto-resets to safe defaults when data freezes:
  - IIE ‚Üí `mean=0.0`, `var=1.0` (blocks actions)
  - Info ‚Üí `faces=0`, `gaze=0` (triggers always-on stop)
- Prevents action spam from frozen sensor values

**4. Optimized Timing**
- `WAIT_AFTER_ACTION`: **3.5s** (ensures rolling window fully refreshes with post-action data)
- `COOLDOWN`: **5.0s** (minimum time between proactive actions)
- Total cycle time: ~9s per action (3.5s wait + 5.0s cooldown + decision overhead)

**Why 3.5s Wait?**
- Action execution time: ~0.5s
- Human reaction time: ~1.0s
- Rolling window refresh: 3.0s (60 samples at 20Hz)
- Safety margin: +0.5s for timing jitter
- The wait ensures post-state measurement contains only post-action data
- Without full refresh, post-state would be contaminated with 50%+ pre-action samples

---

## Console Dashboard

### Purpose
The system provides **real-time visual feedback** through a clean, intuitive console interface. All messages use consistent prefixes and emojis for instant status recognition.

### Logging Convention

**Module Prefixes**:
- `[Actor]` - Embodied Behaviour module (perception, decision-making, action execution)
- `[Actor/IIE]` - IIE Monitor thread
- `[Actor/CTX]` - Context Monitor thread
- `[Actor/INFO]` - Info Monitor thread (faces/gaze)
- `[Actor/AO]` - Always-On Monitor thread
- `[Actor/PRO]` - Proactive thread (learning actions)
- `[Actor/SA]` - Self-Adaptor thread (background behaviors)
- `[Learner]` - Learning module (critic)
- `[Learner/Q]` - Q-Learning updates
- `[Learner/Gate]` - Gatekeeper training

**Status Emojis**:
- ‚úÖ **Success** / Threshold Met / Action Accepted
- ‚è∏Ô∏è **Waiting** / Threshold Not Met / Paused
- üö´ **Gatekeeper Rejection** / Scene Not Opportune
- ‚ö° **Action Execution** / Command Sent
- üì• **Receiving Data** / Experience Received
- üì§ **Sending Data** / Experience Sent
- üíæ **Saving Files** / Model Persistence
- üìä **Measurements** / State Updates
- üé≤ **Exploration** / Epsilon Decay
- üîÑ **Background Action** / Self-Adaptor
- ‚ùå **Error** / Failed Operation
- üõë **Shutdown** / Module Closing
- ‚ñ∂Ô∏è **Thread Started**
- ‚èπÔ∏è **Thread Stopped**

### Example Output

**Startup**:
```
[Actor] ü§ñ EMBODIED BEHAVIOUR MODULE
[Actor] ‚úÖ Ports ready
[Actor] ‚úÖ Always-On active
[Actor] ‚úÖ 6 threads started
[Actor] üìä Thresholds: Œº‚â•0.5, œÉ¬≤<0.1, Œµ=0.80

[Learner] üß† LEARNING MODULE
[Learner] üìÅ Paths: Q=learning_qtable.json, Gate=gate_classifier.pkl
[Learner] ‚úÖ Port ready
[Learner] üíæ Q-table: 2 states
[Learner] üíæ Gatekeeper: 150 trees
[Learner] üìä Q-Learning: Œ±=0.30, Œ≥=0.92
[Learner] üéØ Gatekeeper: Trained (100 trees)
```

**Monitoring Activity**:
```
[Actor/IIE] üìä Œº=0.62, œÉ¬≤=0.08
[Actor/CTX] üî¥Lively
[Actor/INFO] üë§2 üëÅÔ∏è1
```

**Action Decision Flow**:
```
[Actor/PRO] ‚úÖ Thresholds: Œº=0.62, œÉ¬≤=0.08
[Actor/PRO] ‚ö° ao_greet | CTX1 Œº=0.62 Q=0.45 Œµ=0.72
[Actor/PRO] üìä Result: Œº 0.62‚Üí0.68 (+0.06)
[Actor/PRO] üì§ Sent to Learner
```

**Learning Updates**:
```
[Learner] üì• ao_greet
[Learner] Pre‚ÜíPost: Œº 0.62‚Üí0.68, CTX1‚Üí1
[Learner/Q] ‚úÖ R=+0.18, Q: 0.45‚Üí0.51, TD=+0.06
[Learner/Gate] ‚úÖYES Œî=+0.06
[Learner/Gate] üéâ Initialized (100 trees)
[Learner/Gate] Batch: 7‚úÖ 3‚è∏Ô∏è
[Learner/Gate] üíæ Saved (100 trees)
```

**Waiting States** (with reasons):
```
[Actor/PRO] ‚è∏Ô∏è No gaze (faces=2)
[Actor/PRO] ‚è∏Ô∏è Low IIE: Œº=0.42<0.5
[Actor/PRO] ‚è∏Ô∏è Unstable: œÉ¬≤=0.12‚â•0.1
```

**Gatekeeper Rejection** (Phase 3):
```
[Actor/PRO] üö´ Scene not opportune (8s ago)
```

**Always-On State Changes**:
```
[Actor/AO] ‚è∏Ô∏è No faces 120s ‚Üí stopping
[Actor/AO] ‚úÖ Stopped

[Actor/AO] ‚ñ∂Ô∏è Faces detected ‚Üí starting
[Actor/AO] ‚úÖ Started
```

**Self-Adaptor Activity**:
```
[Actor/SA] üîÑ ao_yawn
```

**Errors**:
```
[Actor/PRO] ‚ùå RPC: Connection refused
[Learner] ‚ùå Q-save: Permission denied
[Learner/Gate] ‚ùå Predict: Model file corrupt
```

**Shutdown**:
```
[Actor] üõë Shutting down...
[Actor] ‚úÖ Shutdown complete

[Learner] üõë Shutting down...
[Learner] üíæ Saved: Q=42, Gate=38
[Learner] ‚úÖ Shutdown complete
```

### Benefits

1. **Instant Comprehension**: Emoji status indicators allow quick visual scanning
2. **Module Clarity**: Prefix shows which component is active
3. **Reason Transparency**: Wait/abort messages explain why (threshold values, gaze missing, etc.)
4. **Compact Format**: One-line messages avoid terminal clutter
5. **State Visibility**: Context colors (üîµCalm, üî¥Lively), face/gaze counts (üë§üëÅÔ∏è)
6. **Learning Progress**: Q-values, rewards, gatekeeper labels clearly visible

---

## Architecture

### Module Interaction Flow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    EMBODIED BEHAVIOUR MODULE                     ‚îÇ
‚îÇ                                                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ  ‚îÇ IIE Monitor  ‚îÇ  ‚îÇContext Mon.  ‚îÇ  ‚îÇ Info Monitor ‚îÇ          ‚îÇ
‚îÇ  ‚îÇ (Intention)  ‚îÇ  ‚îÇ (Calm/Lively)‚îÇ  ‚îÇ (Faces/Gaze) ‚îÇ          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ
‚îÇ                         ‚îÇ                                         ‚îÇ
‚îÇ                    Shared State                                   ‚îÇ
‚îÇ              (IIE_mean, IIE_var, ctx,                            ‚îÇ
‚îÇ               num_faces, num_mutual_gaze)                         ‚îÇ
‚îÇ                         ‚îÇ                                         ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                       ‚îÇ
‚îÇ         ‚îÇ                                 ‚îÇ                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ  ‚îÇ  Proactive  ‚îÇ              ‚îÇ Self-Adaptor   ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ   Thread    ‚îÇ              ‚îÇ    Thread      ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ             ‚îÇ              ‚îÇ                ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Load Q    ‚îÇ              ‚îÇ ‚Ä¢ ao_yawn      ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Select    ‚îÇ              ‚îÇ ‚Ä¢ ao_cough     ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Execute   ‚îÇ              ‚îÇ ‚Ä¢ ao_look      ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Measure   ‚îÇ              ‚îÇ   (periodic)   ‚îÇ                ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ         ‚îÇ                                                         ‚îÇ
‚îÇ         ‚îÇ Experience Bottle                                      ‚îÇ
‚îÇ         ‚îÇ (13 fields)                                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
          ‚îÇ YARP Port: /alwayson/embodiedbehaviour/experiences:o
          ‚îÇ
          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                       LEARNING MODULE                            ‚îÇ
‚îÇ                                                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ            Experience Processing Pipeline                   ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ         ‚îÇ                                                         ‚îÇ
‚îÇ         ‚ñº                                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                       ‚îÇ
‚îÇ  ‚îÇ 1. PREDICT  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Log Pred vs  ‚îÇ                       ‚îÇ
‚îÇ  ‚îÇ (ML Models) ‚îÇ         ‚îÇ   Actual     ‚îÇ                       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                       ‚îÇ
‚îÇ         ‚îÇ                                                         ‚îÇ
‚îÇ         ‚ñº                                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                       ‚îÇ
‚îÇ  ‚îÇ 2. COMPUTE  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Q-Learning   ‚îÇ                       ‚îÇ
‚îÇ  ‚îÇ   REWARD    ‚îÇ         ‚îÇ    Update    ‚îÇ                       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                       ‚îÇ
‚îÇ                                  ‚îÇ                                ‚îÇ
‚îÇ                                  ‚ñº                                ‚îÇ
‚îÇ                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                        ‚îÇ
‚îÇ                          ‚îÇ Save Q-table ‚îÇ                        ‚îÇ
‚îÇ                          ‚îÇ   (JSON)     ‚îÇ                        ‚îÇ
‚îÇ                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                        ‚îÇ
‚îÇ         ‚îÇ                                                         ‚îÇ
‚îÇ         ‚ñº                                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                       ‚îÇ
‚îÇ  ‚îÇ 3. TRAIN ML ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Buffer (10x) ‚îÇ                       ‚îÇ
‚îÇ  ‚îÇ   MODELS    ‚îÇ         ‚îÇ IIE + CTX    ‚îÇ                       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Embodied Behaviour Module

### Overview
The actor module that perceives the environment, makes decisions, and executes actions.

### Thread Architecture (6 Threads)

#### 1. IIE Monitor Thread
**Purpose**: Tracks interaction intention estimation

**Input Port**: `/alwayson/embodiedbehaviour/iie:i`

**What it does**:
- Reads IIE distributions for all detected faces
- Each face has: `[mean, variance]`
- Selects the person with **highest mean** (most engaged)
- Updates shared state: `IIE_mean`, `IIE_var`

**Update Rate**: 20 Hz (every 0.05s)

**Example Bottle**:
```
((0 (faceID mean variance std)) (1 (faceID mean variance std)) ...)
```

#### 2. Context Monitor Thread
**Purpose**: Tracks environmental context classification

**Input Port**: `/alwayson/embodiedbehaviour/context:i`

**What it does**:
- Reads context label from STM
- `0` = Calm environment
- `1` = Lively environment
- `-1` = Uncertain
- Updates shared state: `ctx`

**Update Rate**: 10 Hz (every 0.1s)

**Example Bottle**:
```
(episode_id chunk_id label)
‚Üí (42 5 1) means episode 42, chunk 5, lively context
```

#### 3. Info Monitor Thread
**Purpose**: Tracks face count and mutual gaze

**Input Port**: `/alwayson/embodiedbehaviour/info:i`

**What it does**:
- Parses STM info bottle
- Extracts `Faces` and `MutualGaze` counts
- Updates shared state: `num_faces`, `num_mutual_gaze`
- Updates `last_faces_seen_time` (for always-on control)

**Update Rate**: 2 Hz (every 0.5s)

**Bottle Format**:
```
[
  (Time t_vision t_audio t_delta)
  (timestamp Faces n People n Light val Motion val MutualGaze n)
  (timestamp Audio (right left))
]
```

**Parsing Logic**:
```python
vision_list = bottle.get(1).asList()
# Skip timestamp, parse key-value pairs
i = 1
while i < vision_list.size() - 1:
    key = vision_list.get(i).toString()     # "Faces", "MutualGaze", etc.
    value = vision_list.get(i + 1)          # numeric value
    i += 2
```

#### 4. Always-On Monitor Thread
**Purpose**: Auto-stop/start system based on face presence

**What it does**:
- Monitors `num_faces` continuously
- **Stop condition**: No faces for 120 seconds
  - Executes: `echo "exe ao_stop" | yarp rpc /interactionInterface`
  - Sets `alwayson_active = False`
- **Start condition**: Faces detected while stopped
  - Executes: `echo "exe ao_start" | yarp rpc /interactionInterface`
  - Sets `alwayson_active = True`

**Update Rate**: 1 Hz (every 1.0s)

**State Variables**:
```python
alwayson_active: bool          # Current state (on/off)
last_faces_seen_time: float    # Timestamp of last face detection
NO_FACES_TIMEOUT: 120.0        # 2 minutes
```

#### 5. Proactive Thread (Learning Actions)
**Purpose**: Execute learned social behaviors

**Output Port**: `/alwayson/embodiedbehaviour/experiences:o`

**Available Actions**:
```python
PROACTIVE_ACTIONS = [
    "ao_greet",           # Greeting gesture
    "ao_coffee_break",    # Suggest break
    "ao_curious_lean_in"  # Lean forward with interest
]
```

**Decision Loop**:

```
1. Check if always-on is active
   ‚Üí Skip cycle if stopped

2. Capture pre-state snapshot (instant from rolling window)
   ‚Üí Uses 3 seconds of buffered data (60 samples at 20Hz)

3. Check presence conditions:
   ‚úì num_faces > 0
   ‚úì num_mutual_gaze > 0

4. Check intention thresholds:
   ‚úì IIE_mean ‚â• 0.5 (THRESH_MEAN)
   ‚úì IIE_var < 0.1 (THRESH_VAR)

5. Load latest Q-table from disk
   ‚Üí Gets most recent learned values

6. Select action (epsilon-greedy):
   ‚Ä¢ 80% of time (initially): Exploit best Q-value
   ‚Ä¢ 20% of time: Explore random action
   ‚Ä¢ Epsilon decays: 0.8 ‚Üí 0.2 over time

7. Execute action via native YARP RPC:
   yarp.Bottle ‚Üí /interactionInterface (fire-and-forget)

8. Wait 3.5 seconds for action + reaction + sensor integration
   ‚Üí Fixed wait ensures complete rolling window refresh

9. Capture post-state snapshot (instant from rolling window)

10. Send experience to Learning module (13 fields)

11. Log to CSV

12. Decay epsilon: Œµ = max(0.2, Œµ √ó 0.957603)

13. Cooldown 5 seconds
```

**Rolling Window Snapshot** (Non-Blocking, Thread-Safe Noise Filtering):
```python
def _windowed_snapshot():
    """Average state from rolling buffer (instant, 0.0s)"""
    # IIE Monitor continuously fills iie_window at 20Hz
    # Window holds last 60 samples (3 seconds of history)
    
    current = _get_state_snapshot()
    
    # Thread-safe copy (prevents race condition with IIE Monitor)
    with _state_lock:
        window_copy = list(iie_window)
    
    if len(window_copy) < 5:
        return current  # Not enough data yet
    
    # Instant averaging of buffered data (safe to iterate)
    iie_means = [s['mean'] for s in window_copy]
    iie_vars = [s['var'] for s in window_copy]
    
    return {
        'IIE_mean': mean(iie_means),    # 3s average (60 samples)
        'IIE_var': mean(iie_vars),      # 3s average (60 samples)
        'ctx': current['ctx'],          # Current
        'num_faces': current['num_faces'],
        'num_mutual_gaze': current['num_mutual_gaze']
    }
```

**Epsilon-Greedy Selection**:
```python
def _select_action_epsilon_greedy(state_key):
    Q_values = Q[state_key]  # e.g., Q["CTX1"]
    
    if random() < epsilon:
        return random_choice(PROACTIVE_ACTIONS)  # Explore
    else:
        return argmax(Q_values)                  # Exploit
```

**Experience Bottle Format** (13 fields):
```
0:  timestamp (float64)
1:  action (string)
2:  pre_ctx (int8)
3:  pre_IIE_mean (float64)
4:  pre_IIE_var (float64)
5:  post_ctx (int8)
6:  post_IIE_mean (float64)
7:  post_IIE_var (float64)
8:  q_value (float64)
9:  pre_num_faces (int32)
10: pre_num_mutual_gaze (int32)
11: post_num_faces (int32)
12: post_num_mutual_gaze (int32)
```

#### 6. Self-Adaptor Thread (Background Behaviors)
**Purpose**: Periodic natural behaviors (non-learned)

**Available Actions**:
```python
SELF_ADAPTORS = [
    "ao_look_around",  # Look around environment
    "ao_yawn",         # Yawn gesture
    "ao_cough"         # Cough gesture
]
```

**Behavior**:
- **Context-dependent timing**:
  - Calm environment: Every 240 seconds (4 minutes)
  - Lively environment: Every 120 seconds (2 minutes)
- **Random selection**: No learning involved
- **Always active**: Runs even when always-on is stopped
- **Purpose**: Make robot seem alive and natural

**Key Difference from Proactive**:
- ‚úó No threshold checks
- ‚úó No Q-learning
- ‚úó No epsilon-greedy
- ‚úó No experience sent to Learning
- ‚úì Always runs (even when stopped)
- ‚úì Context-adaptive timing

---

## Learning Module

### Overview
The critic module that evaluates actions and improves the policy through Q-learning and ML models.

### Core Components

#### 1. Q-Learning (Value Function)

**Algorithm**: Temporal Difference (TD) Q-Learning

**Q-Table Structure**:
```json
{
  "CTX0": {  // Calm context
    "ao_greet": 0.23,
    "ao_coffee_break": 0.15,
    "ao_curious_lean_in": 0.41
  },
  "CTX1": {  // Lively context
    "ao_greet": 0.52,
    "ao_coffee_break": 0.38,
    "ao_curious_lean_in": 0.35
  }
}
```

**TD Update Rule**:
```python
Q(s, a) ‚Üê Q(s, a) + Œ± [r + Œ≥ ¬∑ max Q(s', a') - Q(s, a)]
                      ‚Üë       ‚Üë           ‚Üë
                   learn   immediate   best future
                   rate    reward      value
```

**Parameters**:
- `Œ± = 0.30`: Learning rate (how much to update)
- `Œ≥ = 0.92`: Discount factor (how much future matters)

**Update Process**:
```python
def _update_q(exp, reward):
    pre_state = f"CTX{exp.pre_ctx}"
    post_state = f"CTX{exp.post_ctx}"
    
    old_q = Q[pre_state][exp.action]
    max_next_q = max(Q[post_state].values())
    
    td_target = reward + 0.92 * max_next_q
    td_error = td_target - old_q
    new_q = old_q + 0.30 * td_error
    
    Q[pre_state][exp.action] = new_q
    return old_q, new_q, td_error
```

**Example Update**:
```
State: CTX1 (Lively)
Action: ao_greet
Old Q: 0.32

Reward received: +0.165
Best future Q (CTX1): 0.45

TD target: 0.165 + 0.92 √ó 0.45 = 0.579
TD error: 0.579 - 0.32 = 0.259
New Q: 0.32 + 0.30 √ó 0.259 = 0.398

Result: Q(CTX1, greet) updated 0.32 ‚Üí 0.40 ‚úì
```

**Persistence**:
- Saved after **every update** (atomic write)
- Format: JSON with temp+rename for safety
- Loaded by Embodied Behaviour before each action

#### 2. Reward Function

**Components** (4 terms + clipping):

```python
reward = (W_DELTA √ó delta_mean) + 
         (W_VAR √ó var_reduction) + 
         (W_LEVEL √ó level_term) - 
         action_cost

# Clip to [-1.0, 1.0] for stability
reward = max(-1.0, min(1.0, reward))
```

**1. Delta Term** (`W_DELTA = 1.0`):
```python
delta_mean = post_IIE_mean - pre_IIE_mean
if abs(delta_mean) < DELTA_EPS:  # 0.05 dead zone
    delta_mean = 0.0
```
- **Purpose**: Reward increasing interaction intention
- **Dead zone**: Filters noise ¬±0.05
- **Example**: 0.65 ‚Üí 0.75 gives +0.10

**2. Variance Term** (`W_VAR = 0.5`):
```python
var_reduction = pre_IIE_var - post_IIE_var
if abs(var_reduction) < VAR_EPS:  # 0.02 dead zone
    var_reduction = 0.0
```
- **Purpose**: Reward making interaction more predictable
- **Dead zone**: Filters noise ¬±0.02
- **Example**: 0.08 ‚Üí 0.05 gives +0.03 (reduction)

**3. Level Term** (`W_LEVEL = 0.5`):
```python
level_term = post_IIE_mean - THRESH_MEAN  # 0.5 threshold
if post_IIE_mean < THRESH_MEAN:
    level_term *= 2.0  # Double penalty!
```
- **Purpose**: Maintain interaction intention above threshold
- **Positive**: Comfortably above 0.5
- **Negative**: Below 0.5 (2√ó penalty)
- **Example**: 
  - IIE = 0.67: level_term = +0.17 (good!)
  - IIE = 0.45: level_term = -0.10 (penalty √ó2)

**4. Action Cost**:
```python
ACTION_COSTS = {
    "ao_greet": 0.08,
    "ao_coffee_break": 0.10,
    "ao_curious_lean_in": 0.06,
}
```
- **Purpose**: Small penalty for energy expenditure
- Prevents action spam

**Complete Example**:
```python
# Scenario: Successful greeting in lively context
pre_IIE_mean:  0.62
post_IIE_mean: 0.78
pre_IIE_var:   0.08
post_IIE_var:  0.06
action:        "ao_greet"

# Compute components:
delta_mean = 0.78 - 0.62 = 0.16 (‚úì above dead zone)
var_reduction = 0.08 - 0.06 = 0.02 (‚úì just at dead zone)
level_term = 0.78 - 0.5 = 0.28 (‚úì well above threshold)
action_cost = 0.08

# Final reward:
reward = 1.0√ó0.16 + 0.5√ó0.02 + 0.5√ó0.28 - 0.08
       = 0.16 + 0.01 + 0.14 - 0.08
       = +0.23 ‚úì Strong positive!

# After clipping (already within [-1.0, 1.0]):
reward = 0.23 (no change)
```

**Reward Philosophy**:
- **Delta**: Reward improvement in interaction intention
- **Variance**: Reward stability
- **Level**: Maintain high interaction intention (don't break interaction!)
- **Cost**: Discourage excessive actions
- **Clipping**: Bound to [-1.0, 1.0] for numerical stability (prevents outliers)

#### 3. Gatekeeper Model (Scene Discriminator)

**Purpose**: Learn to recognize pre-conditions that lead to interaction intention improvement

**Philosophy**: 
- **Learns from RAW OUTCOMES**: Independent of Q-learning reward function
- **Binary Classification**: Did interaction intention actually increase after this action?
- **Physical Reality**: Compares post-IIE vs pre-IIE directly (no weights, no penalties)

**Model**: Binary Classifier
```python
GradientBoostingClassifier(
    n_estimators=100‚Üí200,  # Grows over time
    max_depth=3,
    learning_rate=0.1,
    warm_start=True
)
```

**Input Features** (6D - **PRE-STATE ONLY**):
```python
[
    pre_IIE_mean,           # Engagement level (before action)
    pre_IIE_var,            # Stability (before action)  
    pre_ctx,                # Context (before action)
    pre_num_faces,          # Audience size (before action)
    pre_num_mutual_gaze,    # Attention (before action)
    time_delta              # Time since last action (seconds)
]
# These are the ONLY features used for prediction
# The model decides based on what it sees BEFORE acting
```

**Critical Design**: Uses **only** pre-action values + timing. The model must learn to recognize good opportunities based solely on what it sees *before* acting.

**Output**: Binary Label
- `1` (YES): "This scene historically leads to positive outcomes - ACT"
- `0` (NO): "This scene historically leads to negative outcomes - WAIT"

**Training Label Generation** (Raw Outcome Learning):
```python
# Calculate raw improvement (independent of Q-learning reward)
raw_delta = post_IIE_mean - pre_IIE_mean

# Label based on physical reality (did engagement actually increase?)
if raw_delta > 0.02:  # Small positive threshold
    label = 1  # YES: These pre-conditions led to improvement
else:
    label = 0  # NO: These pre-conditions did not improve engagement

# Key: This is INDEPENDENT of the Q-learning reward function
# Q-learning uses weighted components (delta, variance, level, cost)
# Gatekeeper uses simple post - pre comparison
```

**Training**:
- Buffer size: 10 samples
- When buffer full: train on batch
- Incremental: adds 5 trees per training
- Max trees: 200
- Saves after each training

**Scene Clustering Logic**:
The gatekeeper learns patterns by observing raw outcomes:
- **"YES" Cluster**: Pre-conditions that historically led to +0.02 IIE improvement
  - Example: High IIE (0.6) + 2 faces + 15s elapsed ‚Üí Improved to 0.65
- **"NO" Cluster**: Pre-conditions that did not lead to improvement
  - Example: Low IIE (0.45) + 0 gaze + 5s elapsed ‚Üí Stayed at 0.45 or dropped

**Key Insight**: The model discovers patterns by comparing what happened (post) to what was (pre), not by trusting a reward function's weighted opinion.

**Prediction Interface**:
```python
def predict_decision(pre_IIE_mean, pre_IIE_var, pre_ctx, 
                     pre_num_faces, pre_num_mutual_gaze, time_delta):
    """Returns True (ACT) or False (WAIT)"""
    features = [pre_IIE_mean, pre_IIE_var, pre_ctx, 
                pre_num_faces, pre_num_mutual_gaze, time_delta]
    prediction = gate_model.predict([features])[0]
    return bool(prediction == 1)
```

**Usage in Decision-Making** (Future Integration):
```python
# In embodiedBehaviour.py proactive loop:
if gatekeeper.predict_decision(pre_state, time_since_last_action):
    # Scene matches "winning" cluster ‚Üí Execute action
    execute_action()
else:
    # Scene matches "losing" cluster ‚Üí Wait
    wait_for_better_opportunity()
```

#### 4. Experience Processing Pipeline

**Streamlined Order** (Gatekeeper Architecture):

```python
def _process_experience(exp):
    # 1. CALCULATE TIME DELTA
    time_delta = exp.timestamp - last_exp_timestamp
    
    # WHY: Timing is a critical feature for opportunity recognition
    
    # 2. Q-LEARNING UPDATE
    reward = _compute_reward(exp)
    old_q, new_q, td_error = _update_q(exp, reward)
    _save_qtable()  # Save immediately!
    
    # WHY: Embodied Behaviour loads Q-table before next action
    
    # 3. TRAIN GATEKEEPER (Scene Discriminator)
    label = 1 if reward > 0.05 else 0  # YES/NO scene
    features = _encode_gate_features(exp, time_delta)  # 6D pre-state
    _train_gate_model(features, label, reward)
    
    # WHY: Learn to recognize winning vs losing scenes based on
    #      pre-action conditions and timing
    
    # 4. UPDATE TIMESTAMP
    last_exp_timestamp = exp.timestamp
    
    # WHY: Enable time delta calculation for next experience
```

**Key Differences from Old Architecture**:
- ‚ùå No prediction-before-training (gatekeeper learns from hindsight)
- ‚ùå No regression models (binary classification only)
- ‚ùå No action encoding (timing-based, not action-dependent)
- ‚úì Reward-based labeling (positive ‚Üí YES cluster, negative ‚Üí NO cluster)
- ‚úì Time-aware features (learns when to act, not just what happens)

---

## Communication Protocol

### Port Connections

```
STM ‚Üí Embodied Behaviour:
  /alwayson/stm/info:o ‚Üí /alwayson/embodiedbehaviour/info:i
  /alwayson/stm/context:o ‚Üí /alwayson/embodiedbehaviour/context:i
  
IIE Estimator ‚Üí Embodied Behaviour:
  /iie/estimator:o ‚Üí /alwayson/embodiedbehaviour/iie:i

Embodied Behaviour ‚Üí Learning:
  /alwayson/embodiedbehaviour/experiences:o ‚Üí /alwayson/learning/experiences:i

Robot Interface:
  RPC: /interactionInterface (for action execution)
```

### Experience Bottle (13 Fields)

**Sent by**: Embodied Behaviour  
**Received by**: Learning  
**Frequency**: After each proactive action (~10-20 seconds)

```python
bottle.addFloat64(timestamp)              # 0: When action started
bottle.addString(action)                  # 1: "ao_greet", etc.
bottle.addInt8(pre_ctx)                   # 2: Context before (-1/0/1)
bottle.addFloat64(pre_IIE_mean)           # 3: Engagement before
bottle.addFloat64(pre_IIE_var)            # 4: Variance before
bottle.addInt8(post_ctx)                  # 5: Context after
bottle.addFloat64(post_IIE_mean)          # 6: Engagement after
bottle.addFloat64(post_IIE_var)           # 7: Variance after
bottle.addFloat64(q_value)                # 8: Q-value used
bottle.addInt32(pre_num_faces)            # 9: Faces before
bottle.addInt32(pre_num_mutual_gaze)      # 10: Gaze before
bottle.addInt32(post_num_faces)           # 11: Faces after
bottle.addInt32(post_num_mutual_gaze)     # 12: Gaze after
```

---

## Configuration & Parameters

### Embodied Behaviour

**Action Thresholds**:
```python
THRESH_MEAN = 0.5     # Minimum IIE to act
THRESH_VAR = 0.1      # Maximum variance to act
```

**Timing**:
```python
WAIT_AFTER_ACTION = 3.5          # Seconds for action + reaction + rolling window refresh
COOLDOWN = 5.0                   # Seconds between actions
SELFADAPTOR_PERIOD_CALM = 240.0  # Self-adaptor: calm (4 min)
SELFADAPTOR_PERIOD_LIVELY = 120.0  # Self-adaptor: lively (2 min)
NO_FACES_TIMEOUT = 120.0         # Always-on: stop after (2 min)
```

**Exploration**:
```python
EPSILON = 0.8            # Initial exploration rate
EPSILON_MIN = 0.2        # Minimum exploration rate
EPSILON_DECAY = 0.957603 # Decay per action
```

**Rolling Window** (Non-Blocking Noise Filtering):
```python
WINDOW_SIZE = 60         # Buffer size (3s at 20Hz)
# IIE Monitor feeds buffer continuously
# Snapshot reads instantly from buffer (0.0s latency)
```

**Data Freshness** (Stale Sensor Detection):
```python
IIE_TIMEOUT = 5.0   # Reset to safe defaults after 5s of no updates
INFO_TIMEOUT = 5.0  # Reset to safe defaults after 5s of no updates
# Context has no timeout (deliberate state, not continuous stream)
```

### Learning

**Q-Learning**:
```python
ALPHA = 0.30    # Learning rate
GAMMA = 0.92    # Discount factor
```

**Reward Weights**:
```python
W_DELTA = 1.0   # IIE change weight
W_VAR = 0.5     # Variance reduction weight
W_LEVEL = 0.5   # Engagement level weight
THRESH_MEAN = 0.5  # Threshold for level term
```

**Dead Zones** (Noise Filtering):
```python
DELTA_EPS = 0.05  # Minimum IIE change (¬±0.05)
VAR_EPS = 0.02    # Minimum variance change (¬±0.02)
```

**Gatekeeper Model**:
```python
BUFFER_SIZE = 10                # Training batch size
GATE_MAX_DEPTH = 3              # Tree depth
GATE_N_ESTIMATORS = 100         # Initial trees
GATE_MAX_ESTIMATORS = 200       # Maximum trees
GATE_LEARNING_RATE = 0.1        # Boosting learning rate
REWARD_THRESHOLD = 0.05         # Minimum reward for "YES" label
```

**Action Costs**:
```python
ACTION_COSTS = {
    "ao_greet": 0.08,
    "ao_coffee_break": 0.10,
    "ao_curious_lean_in": 0.06,
}
```

---

## Data Logging

### Embodied Behaviour Logs

#### 1. Proactive Log (`proactive_log.csv`)
**Frequency**: Every proactive action  
**Fields** (14 columns):
```csv
timestamp, proactive_action,
pre_IIE_mean, pre_IIE_var, pre_ctx, pre_num_faces, pre_num_mutual_gaze,
post_IIE_mean, post_IIE_var, post_ctx, post_num_faces, post_num_mutual_gaze,
q_value, epsilon
```

**Purpose**: Track action selection and outcomes

#### 2. Self-Adaptor Log (`selfadaptor_log.csv`)
**Frequency**: Every self-adaptor action  
**Fields** (8 columns):
```csv
timestamp, self_adaptor_action,
pre_IIE_mean, pre_IIE_var, pre_ctx,
post_IIE_mean, post_IIE_var, post_ctx
```

**Purpose**: Track background behaviors

### Learning Logs

#### 1. Q-Learning Log (`qlearning_log.csv`)
**Frequency**: Every Q-update  
**Fields** (14 columns):
```csv
timestamp, proactive_action, pre_state, post_state,
pre_IIE_mean, post_IIE_mean, delta_mean,
pre_IIE_var, post_IIE_var, var_reduction,
reward, old_q, new_q, td_error
```

**Purpose**: Track reward and Q-value evolution

**Key Metrics**:
- `reward`: Total reward received
- `td_error`: Prediction error (how surprising was outcome?)
- `new_q - old_q`: How much Q-value changed

#### 2. Gatekeeper Training Log (`gate_training_log.csv`)
**Frequency**: Every experience  
**Fields** (12 columns):
```csv
timestamp, pre_IIE_mean, pre_IIE_var, pre_ctx,
pre_num_faces, pre_num_mutual_gaze, time_delta,
post_IIE_mean, raw_delta, label, reward_ref, gate_count
```

**Purpose**: Track scene discrimination training

**Key Metrics**:
- `label`: 1 (YES - improved) or 0 (NO - no improvement)
- `raw_delta`: post_IIE - pre_IIE (the actual outcome)
- `post_IIE_mean`: Engagement level after action
- `reward_ref`: Q-learning reward (logged for reference, NOT used in gatekeeper training)
- `gate_count`: Training sample number

**Analysis**:
- Track YES/NO balance (should stabilize around natural improvement rate)
- Identify which pre-conditions lead to raw IIE increases
- Monitor raw_delta distribution (how often does engagement actually improve?)
- Compare `label` (raw outcome) vs `reward_ref` (Q-learning) to see divergence

---

## Reward Function Design

### Design Philosophy

**Goal**: Learn to maintain high engagement while avoiding interaction breakdowns

**Challenges**:
1. **Noise**: IIE measurements flicker ¬±0.03
2. **Threshold**: Must stay above 0.5 to act
3. **Sparse signal**: Changes are often small

**Solutions**:

#### 1. Dead Zones (Noise Filtering)
```python
if abs(delta_mean) < 0.05:
    delta_mean = 0.0  # Ignore tiny changes
```

**Benefit**: Random jitter doesn't create false rewards

#### 2. Windowed Averaging (Noise Filtering)
```python
pre = average_over_3_seconds()   # 30 samples
post = average_over_3_seconds()  # 30 samples
```

**Benefit**: Pre/post measurements are highly stable (~30√ó noise reduction)

#### 3. Level Term (Threshold Awareness)
```python
level_term = post_IIE_mean - 0.5
if post_IIE_mean < 0.5:
    level_term *= 2.0  # Penalty!
```

**Benefit**: 
- Don't break engagement (strong penalty)
- Maintain high IIE (mild reward)

### Reward Examples

**Scenario 1: Maintain engagement**
```python
Pre:  IIE = 0.65, var = 0.08  # From 3s rolling buffer average
Post: IIE = 0.67, var = 0.06  # From 3s rolling buffer average
Action: ao_greet

delta_mean = 0.02 ‚Üí 0.0 (dead zone)
var_reduction = 0.02 ‚Üí 0.02 (at threshold)
level_term = 0.67 - 0.5 = 0.17

reward = 0.0 + 0.5√ó0.02 + 0.5√ó0.17 - 0.08
       = 0.0 + 0.01 + 0.085 - 0.08
       = +0.015 (slightly positive!)
# Clipped: 0.015 (within bounds)
```
‚úì Maintaining high engagement is rewarded

**Scenario 2: Break engagement**
```python
Pre:  IIE = 0.55, var = 0.07
Post: IIE = 0.45, var = 0.09
Action: ao_coffee_break

delta_mean = -0.10
var_reduction = -0.02
level_term = (0.45 - 0.5) √ó 2.0 = -0.10 (√ó2 penalty!)

reward = 1.0√ó(-0.10) + 0.5√ó(-0.02) + 0.5√ó(-0.10) - 0.10
       = -0.10 - 0.01 - 0.05 - 0.10
       = -0.26 (strong penalty!)
# Clipped: -0.26 (within bounds)
```
‚úó Breaking threshold is heavily penalized

**Scenario 3: Boost engagement**
```python
Pre:  IIE = 0.60, var = 0.08
Post: IIE = 0.78, var = 0.05
Action: ao_curious_lean_in

delta_mean = 0.18
var_reduction = 0.03
level_term = 0.78 - 0.5 = 0.28

reward = 1.0√ó0.18 + 0.5√ó0.03 + 0.5√ó0.28 - 0.06
       = 0.18 + 0.015 + 0.14 - 0.06
       = +0.275 (excellent!)
# Clipped: 0.275 (within bounds)
```
‚úì Large improvement + high level = big reward

---

## Learning Progress & Convergence

### Phases of Learning

#### Phase 1: Exploration (Episodes 1-50)
**Epsilon**: 0.8 ‚Üí 0.6

**Behavior**:
- 80% random actions (exploring)
- 20% best known actions (exploiting)
- Building initial Q-table
- High variance in outcomes

**Q-Table Example** (after 20 actions):
```json
{
  "CTX0": {
    "ao_greet": 0.12,
    "ao_coffee_break": -0.05,  // Doesn't work well in calm
    "ao_curious_lean_in": 0.28
  },
  "CTX1": {
    "ao_greet": 0.35,
    "ao_coffee_break": 0.18,
    "ao_curious_lean_in": 0.22
  }
}
```

**ML Models**:
- IIE Model: Not trained yet (need 10 samples)
- Context Model: Not trained yet
- Prediction error: N/A

#### Phase 2: Refinement (Episodes 51-150)
**Epsilon**: 0.6 ‚Üí 0.3

**Behavior**:
- 40% random (still exploring)
- 60% best known (mostly exploiting)
- Q-values converging
- Patterns emerging

**Q-Table Example** (after 100 actions):
```json
{
  "CTX0": {
    "ao_greet": 0.23,
    "ao_coffee_break": 0.15,
    "ao_curious_lean_in": 0.41  // Clear winner in calm
  },
  "CTX1": {
    "ao_greet": 0.52,           // Clear winner in lively
    "ao_coffee_break": 0.38,
    "ao_curious_lean_in": 0.35
  }
}
```

**Gatekeeper Model**:
- Trained 10+ times, 100+ trees
- Learning scene patterns: "High IIE + 15s wait ‚Üí YES"
- Scene discrimination emerging

#### Phase 3: Convergence (Episodes 150+)
**Epsilon**: 0.3 ‚Üí 0.2

**Behavior**:
- 20% random (minimal exploration)
- 80% best known (mostly exploiting)
- Q-values stable
- Confident action selection

**Q-Table Example** (after 300 actions):
```json
{
  "CTX0": {
    "ao_greet": 0.25,
    "ao_coffee_break": 0.18,
    "ao_curious_lean_in": 0.47  // Stable best
  },
  "CTX1": {
    "ao_greet": 0.56,           // Stable best
    "ao_coffee_break": 0.41,
    "ao_curious_lean_in": 0.38
  }
}
```

**Gatekeeper Model**:
- 200 trees, fully trained
- Clear scene clustering: YES (60%) vs NO (40%)
- Recognizes winning patterns:
  - **YES Cluster**: High IIE (>0.6) + Mutual gaze + Time elapsed (>12s)
  - **NO Cluster**: Low IIE (<0.4) OR Too soon (<8s) OR No attention

### Convergence Indicators

**Q-values stabilizing**:
```python
# Watch in qlearning_log.csv
td_error ‚Üí 0 (predictions match reality)
new_q - old_q ‚Üí 0 (no more updates)
```

**Gatekeeper raw outcome learning**:
```python
# Watch in gate_training_log.csv
YES scenes: Pre-conditions where raw_delta > 0.02 (actual improvement)
NO scenes: Pre-conditions where raw_delta ‚â§ 0.02 (no improvement)
Label distribution: Reflects natural improvement rate (varies by interaction quality)
raw_delta: Mean should be positive if actions are generally helpful
```

**Reward increasing**:
```python
# Watch in qlearning_log.csv
mean(reward) increasing over time
variance(reward) decreasing over time
```

**Epsilon decaying**:
```python
# Watch in proactive_log.csv
epsilon: 0.8 ‚Üí 0.6 ‚Üí 0.4 ‚Üí 0.2
```

### Expected Timeline

**Quick learning**: 20-30 actions
- Initial patterns emerge
- Q-values start differentiating

**Competent**: 50-100 actions
- Reliable action selection
- Context-appropriate behaviors

**Expert**: 150-300 actions
- Near-optimal policy
- Confident predictions

---

## Future Integration: Gatekeeper-Gated Actions

### Overview
After the gatekeeper model has collected sufficient training data (200+ experiences, ~Phase 3+), it can be integrated into the proactive action loop to add learned timing intelligence on top of hard-coded thresholds.

### Current Architecture (Phase 1-2)
```
Thresholds Met ‚Üí Load Q-Table ‚Üí Select Action ‚Üí Execute
   (IIE ‚â• 0.5, Var < 0.1, Faces > 0, Gaze > 0)
```

### Future Architecture (Phase 3+)
```
Thresholds Met ‚Üí Gatekeeper Check ‚Üí Load Q-Table ‚Üí Select Action ‚Üí Execute
   (Hard-coded)    (Learned timing)
```

### Integration Point
The gatekeeper will be queried **after** threshold checks but **before** action selection in the proactive thread.

**Cross-Process Solution**: Since `embodiedBehaviour.py` and `learning.py` run as separate YARP modules (different processes), they cannot directly share Python objects. The solution uses **disk-based model sharing**:

1. **Learning module** trains and saves `gate_classifier.pkl` to disk
2. **Embodied Behaviour module** loads the model file when making decisions
3. Both modules access the same file (Learning writes, Actor reads)

**Implementation**:

```python
# ============================================================
# TODO: GATEKEEPER INTEGRATION (PHASE 3+)
# ============================================================
# Uncomment this block after 200+ training samples collected
# Requires: pickle, numpy imports (see top of file)
# Requires: self.last_action_time tracking (see __init__)
# ============================================================

# Calculate time since last action
current_time = time.time()
time_delta = current_time - getattr(self, 'last_action_time', current_time - 10.0)

# Query disk-based gatekeeper model
should_act = self._check_gatekeeper(pre, time_delta)

if not should_act:
    print(f"[Proactive] üö´ Gatekeeper: Scene not opportune (wait {time_delta:.1f}s)")
    time.sleep(2.0)
    continue

print(f"[Proactive] ‚úÖ Gatekeeper: Scene opportune (proceed)")

# Update timestamp for next action
self.last_action_time = time.time()
# ============================================================
```

**Helper Method** (add to `embodiedBehaviour.py`):

```python
def _check_gatekeeper(self, pre, time_delta):
    """Load gatekeeper model from disk and predict 'Should I Act?'
    
    PHASE 3 INTEGRATION: Uncomment this method after 200+ training samples
    
    Args:
        pre: Pre-state snapshot dict
        time_delta: Time since last action (seconds)
    
    Returns:
        bool: True = ACT (scene opportune), False = WAIT (scene not opportune)
    """
    model_path = os.path.join(os.path.dirname(__file__), "gate_classifier.pkl")
    
    # If no model exists yet (early training), default to YES (allow exploration)
    if not os.path.exists(model_path):
        return True
    
    try:
        # Load the trained model from disk (read-only)
        with open(model_path, 'rb') as f:
            model = pickle.load(f)
        
        # Encode features: MUST MATCH learning.py _encode_gate_features() ORDER EXACTLY
        # [pre_IIE_mean, pre_IIE_var, pre_ctx, pre_num_faces, pre_num_mutual_gaze, time_delta]
        features = np.array([[
            pre['IIE_mean'],
            pre['IIE_var'],
            float(pre['ctx']),
            float(pre['num_faces']),
            float(pre['num_mutual_gaze']),
            float(time_delta)
        ]])
        
        # Predict: 1 = YES (opportune scene), 0 = NO (wait)
        prediction = model.predict(features)[0]
        return prediction == 1
    
    except Exception as e:
        print(f"[Gatekeeper] ‚ö†Ô∏è Prediction error: {e}")
        return True  # Default to ACT if model file is corrupt/busy
```

### Benefits
1. **Learned from Physical Reality**: Discovers what actually works (raw IIE changes)
   - Not biased by reward function weights or penalties
   - Learns: "These conditions historically led to +0.05 IIE improvement"
2. **Scene Pattern Recognition**: Identifies improvement-prone vs stagnant scenes
   - **YES Cluster**: Conditions that led to engagement increases
   - **NO Cluster**: Conditions that didn't improve engagement
3. **Reduced Wasted Actions**: Avoids acting in scenes that historically don't improve
4. **Independent Validation**: Can compare gatekeeper decisions to Q-learning to spot issues

### Training Requirements (Before Integration)
- **Minimum samples**: 200 experiences (buffer fills 20 times)
- **Phase**: Episode 100+ (Phase 3 convergence)
- **Gatekeeper convergence**: Clear YES/NO scene clustering visible in `gate_training_log.csv`
- **Label balance**: Stabilized around 60% YES, 40% NO

### Implementation Checklist

**Phase 1-2 (Collect Training Data)**:
- [ ] Run system with threshold-based actions only
- [ ] Collect 200+ experiences with diverse scenes
- [ ] Verify gatekeeper scene clustering in `gate_training_log.csv`
- [ ] Check label balance (should stabilize ~60% YES, 40% NO)
- [ ] Confirm `gate_classifier.pkl` exists and grows to 200 trees

**Phase 3 (Activate Gatekeeper)**:
- [ ] **Add imports** to `embodiedBehaviour.py`:
  ```python
  import pickle
  import numpy as np
  ```
- [ ] **Add timing tracker** to `__init__`:
  ```python
  self.last_action_time = time.time()
  ```
- [ ] **Uncomment `_check_gatekeeper()` method** (already in code, commented)
- [ ] **Uncomment integration block** in proactive loop (already in code, commented)
- [ ] **Test gated actions** in live interaction
- [ ] **Monitor rejection rate**: Expect 20-40% of opportunities rejected with "Scene not opportune"
- [ ] **Compare performance**: Track reward improvement with gatekeeper vs without

**Rollback Plan** (if gatekeeper performs poorly):
- [ ] Re-comment the integration block
- [ ] System reverts to threshold-based decisions
- [ ] Gatekeeper continues training in background (no impact on actions)

### Final Activation Steps

**Current System Status**: ‚úÖ **GREEN** - Code is clean, documented, and logically consistent. Ready to deploy.

**Current Mode**: Phase 1/2 (Data Collection)
- The gatekeeper logic in `embodiedBehaviour.py` is **commented out**
- System runs threshold-based actions only
- Gatekeeper trains passively in background (via `learning.py`)

**To Activate Phase 3 (AI Gating)**:

After collecting 200+ training samples and verifying gatekeeper convergence:

1. **Uncomment Line 56** in `embodiedBehaviour.py`:
   ```python
   self.last_action_time = time.time()
   ```
   *(Enables timing tracker for action intervals)*

2. **Uncomment Lines 407-438** in `embodiedBehaviour.py`:
   ```python
   # ============================================================
   # TODO: GATEKEEPER INTEGRATION (PHASE 3+)
   # ============================================================
   # [Full integration block with time_delta calculation and gatekeeper check]
   # ============================================================
   ```
   *(Enables gatekeeper decision-making in proactive loop)*

3. **Uncomment Lines 530-569** in `embodiedBehaviour.py`:
   ```python
   def _check_gatekeeper(self, pre, time_delta):
       """Load gatekeeper model from disk and predict 'Should I Act?'"""
       # [Full helper method implementation]
   ```
   *(Enables disk-based model loading and prediction)*

**Verification**:
- System will start logging: `"[Actor/PRO] ‚úÖ Scene opportune"` or `"[Actor/PRO] üö´ Scene not opportune (Xs ago)"`
- Expect 20-40% rejection rate in typical interaction scenarios
- Monitor `gate_training_log.csv` for decision patterns
- Console dashboard will show gatekeeper decisions in real-time

**Rollback** (if needed):
- Re-comment the three sections above
- System reverts to Phase 1/2 threshold-based decisions
- No data loss, gatekeeper continues training

### Why Wait Until Phase 3?
- **Phase 1-2**: Gatekeeper has insufficient data, predictions unreliable
- **Phase 3**: Model has seen diverse scenes, clusters formed, timing patterns learned
- **Risk of early integration**: Random rejections, disrupted exploration
- **Benefit of delayed integration**: Stable timing intelligence, proven patterns

### Technical Notes

**Cross-Process Architecture**:
- `embodiedBehaviour.py` and `learning.py` are separate YARP modules (different OS processes)
- Cannot share Python objects directly (no shared memory)
- Solution: File-based model sharing via `gate_classifier.pkl`

**File Access Pattern**:
- **Learning module**: Writes `gate_classifier.pkl` after every 10 training samples
- **Embodied Behaviour**: Reads `gate_classifier.pkl` before each decision
- **Concurrency**: Read-only access from Actor is safe (no write conflicts)
- **Failure mode**: If file is corrupted/busy, defaults to `True` (allow action)

**Feature Encoding Consistency**:
- **CRITICAL**: Both modules must encode features in **identical order**
- Learning module defines order in `_encode_gate_features()`:
  ```python
  [pre_IIE_mean, pre_IIE_var, pre_ctx, pre_num_faces, pre_num_mutual_gaze, time_delta]
  ```
- Actor module must match exactly in `_check_gatekeeper()` (see helper method above)
- Mismatch will cause prediction errors

**Performance Impact**:
- Model loading: ~5-10ms per decision (negligible vs 9s action cycle)
- Prediction: ~1-2ms (GradientBoosting inference)
- Total overhead: <15ms per action (~0.2% of cycle time)

---

## Troubleshooting

### Common Issues

#### 1. No Actions Executed
**Symptoms**: Proactive thread never acts

**Checks**:
```python
# Check thresholds:
IIE_mean ‚â• 0.5?
IIE_var < 0.1?
num_faces > 0?
num_mutual_gaze > 0?
alwayson_active == True?
```

**Solutions**:
- Lower thresholds temporarily
- Check IIE estimator output
- Verify STM info bottle format

#### 2. Q-Values Not Updating
**Symptoms**: Q-table stays at 0.0

**Checks**:
```python
# In Learning module:
exp.pre_ctx != -1?
exp.post_ctx != -1?
reward != 0.0?
```

**Solutions**:
- Check experience bottle format
- Verify context is being detected
- Check reward components

#### 3. High Prediction Errors
**Symptoms**: ML models not learning

**Checks**:
```python
# In model_prediction_log.csv:
prediction_error > 0.20 after 100 samples?
```

**Solutions**:
- Check feature encoding
- Verify StandardScaler is fitted
- Increase buffer size or tree count

#### 4. Always-On Stuck
**Symptoms**: Won't start/stop

**Checks**:
```python
# Check RPC interface:
yarp rpc /interactionInterface
exe ao_start  # Manual test
```

**Solutions**:
- Verify interaction interface running
- Check RPC command format
- Review always-on monitor logs

---

## Appendix: Mathematical Notation

### Q-Learning
```
Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ max Q(s',a') - Q(s,a)]
                      a'

Where:
  Q(s,a): Value of action a in state s
  Œ±: Learning rate (0.30)
  r: Reward received
  Œ≥: Discount factor (0.92)
  s': Next state
  max Q(s',a'): Best future value
     a'
```

### Reward Function
```
R_raw = W_Œî¬∑ŒîŒº + W_œÉ¬∑ŒîœÉ¬≤ + W_L¬∑L - C

R = clip(R_raw, -1.0, 1.0)  # Bounded for stability

Where:
  ŒîŒº = {Œº_post - Œº_pre  if |ŒîŒº| ‚â• 0.05
       {0                otherwise
  
  ŒîœÉ¬≤ = {œÉ¬≤_pre - œÉ¬≤_post  if |ŒîœÉ¬≤| ‚â• 0.02
        {0                  otherwise
  
  L = {(Œº_post - 0.5)      if Œº_post ‚â• 0.5
      {2(Œº_post - 0.5)      if Œº_post < 0.5
  
  C = action cost
  
  W_Œî = 1.0, W_œÉ = 0.5, W_L = 0.5
```

### Epsilon-Greedy
```
œÄ(s) = {argmax Q(s,a)    with probability 1-Œµ
       {random action     with probability Œµ
        a

Œµ_t = max(Œµ_min, Œµ_0 ¬∑ Œª^t)

Where:
  Œµ_0 = 0.8 (initial)
  Œµ_min = 0.2 (minimum)
  Œª = 0.957603 (decay rate)
  t = action count
```
